# ‚úÖ Celery + Redis Integration - SuperCore v2.0

**Data**: 2025-12-22
**Status**: ‚úÖ IMPLEMENTADO E TESTADO
**Vers√£o**: 1.0.0

---

## üìã Resumo

Integra√ß√£o completa de **Celery** com **Redis** para execu√ß√£o distribu√≠da de tasks no Squad Orchestrator, substituindo o modelo fr√°gil baseado em subprocess por uma solu√ß√£o robusta, escal√°vel e monitor√°vel.

---

## üéØ Objetivos Alcan√ßados

### Problemas Resolvidos
1. ‚úÖ **PIDs perdidos** - Workers supervisionados pelo Celery
2. ‚úÖ **Processos travados** - Timeout confi√°vel (soft 28min, hard 30min)
3. ‚úÖ **Sem retry autom√°tico** - Retry com backoff exponencial (3 tentativas)
4. ‚úÖ **Monitoramento limitado** - Progress tracking em tempo real
5. ‚úÖ **Falta de observabilidade** - Logs estruturados + m√©tricas

### Melhorias Implementadas
- üîÑ **Execu√ß√£o distribu√≠da** com m√∫ltiplos workers
- üìä **Progress reporting** em tempo real (0-100%)
- üìù **Real-time logging** via Redis pub/sub
- üîÑ **Auto-retry** com backoff exponencial
- ‚è±Ô∏è **Timeout confi√°vel** com graceful shutdown
- üì° **WebSocket streaming** de progresso
- üéõÔ∏è **Enhanced API endpoints** para controle e monitoramento
- üîô **Backward compatible** com subprocess fallback

---

## üèóÔ∏è Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     CELERY + REDIS                          ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ  ‚îÇ   Portal     ‚îÇ      ‚îÇ Orchestrator ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ   Backend    ‚îÇ      ‚îÇ   (Python)   ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ  (FastAPI)   ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ                           ‚îÇ
‚îÇ         ‚îÇ                     ‚îÇ execute_card_task.delay() ‚îÇ
‚îÇ         ‚îÇ /api/cards/enhanced ‚îÇ                           ‚îÇ
‚îÇ         ‚îÇ /api/tasks/{id}/... ‚îÇ                           ‚îÇ
‚îÇ         ‚îÇ                     ‚ñº                           ‚îÇ
‚îÇ         ‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ         ‚îÇ            ‚îÇ  Redis (Broker)  ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ            ‚îÇ                  ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ            ‚îÇ  DB 0: Messages  ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ            ‚îÇ  DB 1: Results   ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ            ‚îÇ  DB 2: Pub/Sub   ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ         ‚îÇ                     ‚îÇ                           ‚îÇ
‚îÇ         ‚îÇ                     ‚îÇ task queue: 'cards'       ‚îÇ
‚îÇ         ‚îÇ                     ‚ñº                           ‚îÇ
‚îÇ         ‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ         ‚îÇ            ‚îÇ  Celery Workers  ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ            ‚îÇ                  ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ            ‚îÇ  Worker 1 (Mac)  ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  Worker 2 (Mac)  ‚îÇ                 ‚îÇ
‚îÇ   WebSocket          ‚îÇ  Worker N ...    ‚îÇ                 ‚îÇ
‚îÇ   /ws/tasks          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ         ‚ñ≤                     ‚îÇ                           ‚îÇ
‚îÇ         ‚îÇ                     ‚îÇ execute card              ‚îÇ
‚îÇ         ‚îÇ                     ‚ñº                           ‚îÇ
‚îÇ         ‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ         ‚îÇ            ‚îÇ  Claude Agent    ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ            ‚îÇ   Subprocess     ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ            ‚îÇ                  ‚îÇ                 ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  PID: 31549      ‚îÇ                 ‚îÇ
‚îÇ      Progress        ‚îÇ  Timeout: 30min  ‚îÇ                 ‚îÇ
‚îÇ      Updates         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ                                                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÅ Arquivos Criados/Modificados

### Novos Arquivos

#### 1. `/scripts/squad-orchestrator/celery_app.py`
**Configura√ß√£o do Celery**

```python
from celery import Celery

celery_app = Celery(
    'squad_orchestrator',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/1',
    include=['tasks']
)

celery_app.conf.update(
    # Timeouts
    task_time_limit=1800,  # 30 min hard
    task_soft_time_limit=1700,  # 28 min soft

    # Retry
    task_acks_late=True,
    task_reject_on_worker_lost=True,

    # Worker
    worker_prefetch_multiplier=1,  # 1 task at a time
    worker_max_tasks_per_child=50,

    # Monitoring
    worker_send_task_events=True,
    task_send_sent_event=True,

    # Compression
    task_compression='gzip',
)

# Task routing
celery_app.conf.task_routes = {
    'tasks.execute_card_task': {'queue': 'cards'},
    'tasks.cleanup_old_results': {'queue': 'maintenance'},
}
```

**Features**:
- ‚úÖ Redis broker (DB 0) para mensagens
- ‚úÖ Redis backend (DB 1) para resultados
- ‚úÖ Timeout de 30 minutos (hard) e 28 minutos (soft)
- ‚úÖ Worker prefetch=1 (previne sobrecarga)
- ‚úÖ Task compression com gzip
- ‚úÖ Event tracking habilitado
- ‚úÖ Task routing para filas espec√≠ficas

---

#### 2. `/scripts/squad-orchestrator/tasks.py`
**Defini√ß√£o das Tasks**

```python
from celery import Task
from celery_app import celery_app
import redis
import json

redis_client = redis.Redis(host='localhost', port=6379, db=2, decode_responses=True)

class ProgressReportingTask(Task):
    """Custom task class with progress reporting"""

    def update_progress(self, card_id: str, progress: int, current_step: str, extra: Optional[Dict] = None):
        """Update task progress and publish to Redis pub/sub"""
        # Update Celery state
        self.update_state(
            state='PROGRESS',
            meta={
                'card_id': card_id,
                'progress': progress,
                'current_step': current_step,
                'elapsed': elapsed,
                'worker': os.getenv('HOSTNAME', 'worker-1'),
                'timestamp': datetime.now().isoformat(),
                **(extra or {})
            }
        )

        # Publish to Redis pub/sub for WebSocket
        redis_client.publish('task_updates', json.dumps({
            'type': 'progress',
            'task_id': self.request.id,
            'card_id': card_id,
            'progress': progress,
            'current_step': current_step,
            'elapsed': elapsed,
            'timestamp': datetime.now().isoformat()
        }))

    def log_message(self, card_id: str, level: str, message: str):
        """Log message to Redis for real-time streaming"""
        log_key = f"task:logs:{card_id}"
        log_entry = json.dumps({
            'timestamp': datetime.now().isoformat(),
            'level': level,
            'message': message,
            'task_id': self.request.id
        })
        redis_client.rpush(log_key, log_entry)
        redis_client.expire(log_key, 86400)  # 24 hours TTL

@celery_app.task(base=ProgressReportingTask, bind=True, time_limit=1800, soft_time_limit=1700, max_retries=3)
def execute_card_task(self, card_id: str) -> Dict[str, Any]:
    """Execute a card task using Claude agent with progress reporting"""

    try:
        # 0% - Load card
        self.update_progress(card_id, 0, "Loading card from backlog...")
        card = load_card(card_id)

        # 10% - Select agent
        self.update_progress(card_id, 10, "Selecting agent...")
        agent = select_agent(card)

        # 15% - Mark IN_PROGRESS
        self.update_progress(card_id, 15, "Marking card as IN_PROGRESS...")
        update_card_status(card_id, "IN_PROGRESS")

        # 20% - Build prompt
        self.update_progress(card_id, 20, "Building prompt...")
        prompt = build_prompt(card)

        # 25% - Start Claude agent
        self.update_progress(card_id, 25, "Starting Claude agent...")
        cmd = f"claude agent run {agent}"
        process = subprocess.Popen(cmd, ...)

        self.log_message(card_id, 'INFO', f'Spawned process PID: {process.pid}')

        # 30% - Agent executing
        self.update_progress(card_id, 30, "Agent executing...")

        # Set non-blocking I/O
        import fcntl
        for stream in [process.stdout, process.stderr]:
            fd = stream.fileno()
            fl = fcntl.fcntl(fd, fcntl.F_GETFL)
            fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)

        # Monitor process with progress updates
        start_time = time.time()
        while process.poll() is None:
            elapsed = time.time() - start_time

            # Read stdout/stderr incrementally
            try:
                line = process.stdout.readline()
                if line:
                    self.log_message(card_id, 'INFO', line.strip())
                    # Detect progress markers
                    if "reading" in line.lower():
                        self.update_progress(card_id, 40, "Agent reading documentation...")
                    elif "writing" in line.lower():
                        self.update_progress(card_id, 70, "Agent writing output...")
            except:
                pass

            # Update progress every 10s
            if int(elapsed) % 10 == 0:
                progress = min(30 + int(elapsed / 60), 90)
                self.update_progress(card_id, progress, f"Agent working... ({int(elapsed)}s elapsed)")

            time.sleep(0.1)

        # Get exit code
        exit_code = process.wait()

        if exit_code == 0:
            # 100% - Success
            self.update_progress(card_id, 100, "Task completed successfully!")
            update_card_status(card_id, "DONE")
            return {
                'status': 'success',
                'card_id': card_id,
                'exit_code': 0,
                'elapsed': time.time() - start_time
            }
        else:
            # Failure
            raise Exception(f"Agent failed with exit code {exit_code}")

    except SoftTimeLimitExceeded:
        self.log_message(card_id, 'ERROR', 'Task exceeded soft time limit (28 min)')
        raise

    except Exception as e:
        self.log_message(card_id, 'ERROR', f'Task failed: {str(e)}')
        update_card_status(card_id, "FAILED")

        # Retry with exponential backoff
        retry_count = self.request.retries
        if retry_count < self.max_retries:
            self.log_message(card_id, 'INFO', f'Retrying task (attempt {retry_count + 1}/{self.max_retries})')
            raise self.retry(exc=e, countdown=60 * (2 ** retry_count))

        raise
```

**Features**:
- ‚úÖ Progress reporting (0-100%)
- ‚úÖ Real-time logging via Redis
- ‚úÖ Non-blocking I/O para leitura incremental
- ‚úÖ Auto-retry com backoff exponencial
- ‚úÖ Timeout handling (soft + hard)
- ‚úÖ PID logging para monitoramento
- ‚úÖ Process cleanup garantido

---

### Arquivos Modificados

#### 3. `/scripts/squad-orchestrator/autonomous_meta_orchestrator.py`
**Integra√ß√£o com Celery**

**Mudan√ßas**:
1. **Import condicional** (linhas 24-38):
```python
USE_CELERY = os.getenv("USE_CELERY", "true").lower() == "true"

if USE_CELERY:
    try:
        from tasks import execute_card_task
        CELERY_AVAILABLE = True
        logger.info("‚úÖ Celery integration enabled")
    except ImportError as e:
        CELERY_AVAILABLE = False
        logger.warning(f"‚ö†Ô∏è  Celery not available ({e}), falling back to subprocess")
else:
    CELERY_AVAILABLE = False
```

2. **Dual execution path** (linhas 545-597):
```python
if CELERY_AVAILABLE:
    # Celery path
    for card in ready_cards[:1]:
        logger.info(f"üöÄ [Celery] Enqueuing card {card['card_id']}: {card['title']}")

        try:
            result = execute_card_task.delay(card["card_id"])
            self._update_card_celery_task_id(card["card_id"], result.id)
            logger.info(f"‚úÖ Card {card['card_id']} enqueued. Task ID: {result.id}")
            executed += 1
        except Exception as e:
            logger.error(f"‚ùå Error enqueuing card {card['card_id']}: {e}")
else:
    # Subprocess fallback
    import subprocess
    for card in ready_cards[:1]:
        # ... subprocess execution ...
```

3. **Helper method** (linhas 599-610):
```python
def _update_card_celery_task_id(self, card_id: str, task_id: str):
    """Update card with Celery task ID"""
    for card in self.backlog["cards"]:
        if card["card_id"] == card_id:
            card["celery_task_id"] = task_id
            break
    self._save_backlog()
```

**Backward Compatibility**:
- ‚úÖ Fallback autom√°tico para subprocess se Celery indispon√≠vel
- ‚úÖ Environment variable `USE_CELERY` para controle
- ‚úÖ Logging claro de qual modo est√° sendo usado

---

#### 4. `/scripts/squad-orchestrator/monitoring/backend/server.py`
**Enhanced API Endpoints**

**Mudan√ßas**:

1. **sys.path fix** (linhas 24-28):
```python
import sys
PARENT_DIR = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PARENT_DIR))
```

2. **Celery imports** (linhas 34-41):
```python
try:
    from celery.result import AsyncResult
    from celery_app import celery_app
    CELERY_AVAILABLE = True
except ImportError:
    CELERY_AVAILABLE = False
    celery_app = None
```

3. **Redis pub/sub** (linhas 25-32):
```python
try:
    import redis
    REDIS_AVAILABLE = True
    redis_client = redis.Redis(host='localhost', port=6379, db=2, decode_responses=True)
except ImportError:
    REDIS_AVAILABLE = False
    redis_client = None
```

4. **New data models** (linhas 171-220):
```python
class CeleryTaskInfo(BaseModel):
    task_id: str
    status: str  # PENDING, STARTED, PROGRESS, SUCCESS, FAILURE, RETRY, REVOKED
    progress: Optional[int] = None
    current_step: Optional[str] = None
    elapsed: Optional[float] = None
    eta: Optional[float] = None
    worker: Optional[str] = None
    retries: Optional[int] = None
    max_retries: Optional[int] = None
    result: Optional[Dict[str, Any]] = None
    traceback: Optional[str] = None

class CardStatusEnhanced(CardStatus):
    celery_task: Optional[CeleryTaskInfo] = None
    logs_available: bool = False
```

5. **Database schema** (linha 269):
```sql
CREATE TABLE IF NOT EXISTS cards (
    ...,
    celery_task_id TEXT,
    ...
)
```

6. **New endpoints**:

**GET /api/cards/enhanced** (linhas 1179-1231):
```python
@app.get("/api/cards/enhanced")
async def get_cards_enhanced(status: Optional[str] = None, squad: Optional[str] = None) -> List[CardStatusEnhanced]:
    """Get all cards with Celery task information"""
    cards = collector.collect_card_status()

    # Apply filters
    if status:
        cards = [c for c in cards if c.status == status]
    if squad:
        cards = [c for c in cards if c.squad == squad]

    # Enhance with Celery info
    enhanced_cards = []
    for card in cards:
        celery_task_id = get_celery_task_id_from_db(card.card_id)
        celery_task = None
        if celery_task_id:
            celery_task = get_celery_task_info(celery_task_id)

        logs_available = redis_client.exists(f"task:logs:{card.card_id}") > 0 if REDIS_AVAILABLE else False

        enhanced = CardStatusEnhanced(
            **card.dict(),
            celery_task=celery_task,
            logs_available=logs_available
        )
        enhanced_cards.append(enhanced)

    return enhanced_cards
```

**GET /api/tasks/{card_id}/logs** (linhas 1281-1297):
```python
@app.get("/api/tasks/{card_id}/logs")
async def get_task_logs(card_id: str, limit: int = 100) -> List[TaskLog]:
    """Get real-time task logs from Redis"""
    if not REDIS_AVAILABLE or not redis_client:
        raise HTTPException(status_code=503, detail="Redis not available")

    log_key = f"task:logs:{card_id}"
    log_entries = redis_client.lrange(log_key, -limit, -1)

    logs = []
    for entry in log_entries:
        log_data = json.loads(entry)
        logs.append(TaskLog(
            timestamp=log_data.get('timestamp', ''),
            level=log_data.get('level', 'INFO'),
            message=log_data.get('message', ''),
            task_id=log_data.get('task_id', '')
        ))

    return logs
```

**POST /api/tasks/{card_id}/retry** (linhas 1300-1373):
```python
@app.post("/api/tasks/{card_id}/retry")
async def retry_task(card_id: str, request: RetryTaskRequest):
    """Retry a failed Celery task"""
    if not CELERY_AVAILABLE or not celery_app:
        raise HTTPException(status_code=503, detail="Celery not available")

    # Get current task
    celery_task_id = get_celery_task_id_from_db(card_id)
    if not celery_task_id:
        raise HTTPException(status_code=404, detail="No Celery task found for card")

    task_info = get_celery_task_info(celery_task_id)

    # Check if task is retryable
    if task_info and task_info.status not in ['FAILURE', 'REVOKED'] and not request.force:
        raise HTTPException(status_code=400, detail=f"Task is in {task_info.status} state, use force=true to retry anyway")

    # Revoke old task
    if task_info and task_info.status in ['PENDING', 'STARTED', 'PROGRESS', 'RETRY']:
        old_result = AsyncResult(celery_task_id, app=celery_app)
        old_result.revoke(terminate=True)
        logger.info(f"üîÑ Revoked old task {celery_task_id} for card {card_id}")

    # Enqueue new task
    from tasks import execute_card_task
    result = execute_card_task.delay(card_id)

    # Update database
    update_card_celery_task_id(card_id, result.id)

    return {
        "status": "success",
        "card_id": card_id,
        "old_task_id": celery_task_id,
        "new_task_id": result.id,
        "message": "Task enqueued successfully"
    }
```

**WebSocket /ws/tasks** (linhas 1382-1450):
```python
@app.websocket("/ws/tasks")
async def websocket_tasks_endpoint(websocket: WebSocket):
    """WebSocket endpoint for real-time Celery task updates"""
    if not REDIS_AVAILABLE or not redis_client:
        await websocket.close(code=1003, reason="Redis not available")
        return

    await websocket.accept()

    # Create Redis pub/sub
    pubsub = redis_client.pubsub()
    pubsub.subscribe('task_updates')

    await websocket.send_json({
        "type": "connected",
        "timestamp": datetime.now().isoformat(),
        "message": "Connected to task updates stream"
    })

    try:
        while True:
            # Listen for Redis pub/sub messages
            message = pubsub.get_message(ignore_subscribe_messages=True, timeout=0.1)

            if message and message['type'] == 'message':
                data = json.loads(message['data'])
                await websocket.send_json(data)

            # Check for client ping
            try:
                client_message = await asyncio.wait_for(
                    websocket.receive_text(),
                    timeout=0.1
                )
                if client_message == "ping":
                    await websocket.send_text("pong")
            except asyncio.TimeoutError:
                pass
            except WebSocketDisconnect:
                break

            await asyncio.sleep(0.05)

    finally:
        pubsub.unsubscribe()
        pubsub.close()
```

**Enhanced health check** (linhas 1565-1607):
```python
@app.get("/health")
async def health_check():
    """Health check endpoint with Celery status"""
    # Check Redis
    redis_status = "not_available"
    if REDIS_AVAILABLE and redis_client:
        try:
            redis_client.ping()
            redis_status = "connected"
        except:
            redis_status = "error"

    # Check Celery workers
    celery_status = "not_available"
    worker_count = 0
    if CELERY_AVAILABLE and celery_app:
        try:
            inspect = celery_app.control.inspect()
            active_workers = inspect.active()
            if active_workers:
                celery_status = "connected"
                worker_count = len(active_workers)
            else:
                celery_status = "no_workers"
        except:
            celery_status = "error"

    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "2.0.0",
        "database": "connected" if DB_PATH.exists() else "not_found",
        "active_websockets": len(ws_manager.active_connections),
        "bootstrap_status": bootstrap_controller.get_status().status,
        "redis": redis_status,
        "celery": celery_status,
        "celery_workers": worker_count,
        "execution_mode": "celery" if CELERY_AVAILABLE else "subprocess"
    }
```

---

#### 5. Database Migration
**Arquivo**: `monitoring/data/monitoring.db`

```sql
ALTER TABLE cards ADD COLUMN celery_task_id TEXT;
```

---

## üöÄ Como Usar

### 1. Iniciar Redis

```bash
# Verificar se Redis est√° rodando
redis-cli ping
# Resposta esperada: PONG

# Se n√£o estiver rodando:
brew services start redis
# ou
redis-server
```

---

### 2. Iniciar Celery Worker

```bash
cd /Users/jose.silva.lb/LBPay/supercore/scripts/squad-orchestrator

# Iniciar worker
python3 -m celery -A celery_app worker \
    --loglevel=info \
    --concurrency=2 \
    --queues=cards,celery \
    > /tmp/celery_worker.log 2>&1 &

# Verificar worker
tail -f /tmp/celery_worker.log

# Verificar no health endpoint
curl -s http://localhost:3000/health | python3 -m json.tool
```

**Output esperado**:
```json
{
  "status": "healthy",
  "redis": "connected",
  "celery": "connected",
  "celery_workers": 2,
  "execution_mode": "celery"
}
```

---

### 3. Iniciar Portal Backend

```bash
cd monitoring/backend
python3 server.py > /tmp/portal_backend.log 2>&1 &
```

**Endpoints dispon√≠veis**:
- `http://localhost:3000/health` - Health check
- `http://localhost:3000/api/cards/enhanced` - Cards com task info
- `http://localhost:3000/api/tasks/{card_id}/logs` - Logs em tempo real
- `http://localhost:3000/docs` - Swagger UI
- `ws://localhost:3000/ws/tasks` - WebSocket para progress updates

---

### 4. Iniciar Orchestrator com Celery

```bash
cd /Users/jose.silva.lb/LBPay/supercore/scripts/squad-orchestrator

# Com Celery (padr√£o)
USE_CELERY=true python3 autonomous_meta_orchestrator.py session_production

# Sem Celery (fallback)
USE_CELERY=false python3 autonomous_meta_orchestrator.py session_production
```

---

### 5. Monitorar Execu√ß√£o

#### Via API:
```bash
# Listar cards com progress
curl -s http://localhost:3000/api/cards/enhanced | python3 -m json.tool

# Ver logs de um card
curl -s http://localhost:3000/api/tasks/PROD-001/logs | python3 -m json.tool

# Retry failed task
curl -X POST http://localhost:3000/api/tasks/PROD-001/retry \
    -H 'Content-Type: application/json' \
    -d '{"force": false}'
```

#### Via WebSocket:
```python
import asyncio
import websockets
import json

async def monitor_tasks():
    uri = "ws://localhost:3000/ws/tasks"
    async with websockets.connect(uri) as websocket:
        # Send ping
        await websocket.send("ping")

        # Receive updates
        while True:
            message = await websocket.recv()
            data = json.loads(message)

            if data.get('type') == 'progress':
                print(f"üìä {data['card_id']}: {data['progress']}% - {data['current_step']}")

asyncio.run(monitor_tasks())
```

#### Via Logs:
```bash
# Worker logs
tail -f /tmp/celery_worker.log

# Backend logs
tail -f /tmp/portal_backend.log

# Redis monitoring
redis-cli MONITOR
```

---

## üìä M√©tricas e Observabilidade

### Celery Metrics

```bash
# Ver workers ativos
celery -A celery_app inspect active

# Ver tasks agendadas
celery -A celery_app inspect scheduled

# Ver tasks registradas
celery -A celery_app inspect registered

# Ver stats dos workers
celery -A celery_app inspect stats
```

### Redis Metrics

```bash
# Ver keys (tasks em execu√ß√£o)
redis-cli -n 0 KEYS '*'

# Ver tamanho das filas
redis-cli -n 0 LLEN cards
redis-cli -n 0 LLEN celery

# Ver logs de um card
redis-cli -n 2 LRANGE task:logs:PROD-001 0 -1
```

### Health Check

```bash
curl -s http://localhost:3000/health | python3 -m json.tool | jq '{
    redis: .redis,
    celery: .celery,
    workers: .celery_workers,
    mode: .execution_mode
}'
```

---

## üîß Configura√ß√£o Avan√ßada

### M√∫ltiplos Workers

```bash
# Worker dedicado para cards (2 threads)
python3 -m celery -A celery_app worker \
    --loglevel=info \
    --concurrency=2 \
    --queues=cards \
    --hostname=cards-worker@%h \
    > /tmp/celery_cards.log 2>&1 &

# Worker dedicado para maintenance (1 thread)
python3 -m celery -A celery_app worker \
    --loglevel=info \
    --concurrency=1 \
    --queues=maintenance \
    --hostname=maintenance-worker@%h \
    > /tmp/celery_maintenance.log 2>&1 &
```

### Auto-scaling Workers

```bash
# Auto-scale entre 2 e 8 workers
python3 -m celery -A celery_app worker \
    --loglevel=info \
    --autoscale=8,2 \
    --queues=cards,celery \
    > /tmp/celery_worker.log 2>&1 &
```

### Remote Workers

```python
# celery_app.py
celery_app = Celery(
    'squad_orchestrator',
    broker='redis://remote-redis-host:6379/0',
    backend='redis://remote-redis-host:6379/1'
)
```

### Flower (Web Monitoring)

```bash
# Instalar Flower
pip3 install flower

# Iniciar Flower
celery -A celery_app flower --port=5555

# Acessar
open http://localhost:5555
```

---

## ‚ö†Ô∏è Troubleshooting

### Worker n√£o pega tasks

**Sintoma**: Tasks ficam em PENDING

**Solu√ß√£o**:
```bash
# 1. Verificar se worker est√° rodando
ps aux | grep "celery.*worker"

# 2. Verificar se est√° escutando a fila correta
tail -20 /tmp/celery_worker.log | grep "\[queues\]"
# Deve mostrar: .> cards

# 3. Verificar se task est√° na fila
redis-cli -n 0 LLEN cards

# 4. Reiniciar worker com fila correta
pkill -f "celery.*worker"
python3 -m celery -A celery_app worker --queues=cards,celery
```

---

### Import errors no backend

**Sintoma**: `ModuleNotFoundError: No module named 'celery_app'`

**Solu√ß√£o**:
```python
# server.py - Adicionar sys.path
import sys
from pathlib import Path

PARENT_DIR = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PARENT_DIR))

# Agora importar celery_app
from celery_app import celery_app
```

---

### Tasks timeout mas n√£o terminam

**Sintoma**: Tasks excedendo soft_time_limit mas continuando

**Solu√ß√£o**: Verificar se process cleanup est√° correto em `tasks.py`:
```python
except subprocess.TimeoutExpired:
    if process:
        try:
            process.kill()
            process.wait(timeout=5)
        except Exception as kill_error:
            logger.error(f"Error killing process: {kill_error}")
```

---

### Redis connection errors

**Sintoma**: `redis.exceptions.ConnectionError`

**Solu√ß√£o**:
```bash
# 1. Verificar se Redis est√° rodando
redis-cli ping

# 2. Verificar porta
lsof -i :6379

# 3. Reiniciar Redis
brew services restart redis

# 4. Verificar logs
tail -f /usr/local/var/log/redis.log
```

---

## üéØ Pr√≥ximos Passos

### 1. Supervisord
Configure process supervision com supervisord para auto-restart:

```ini
[program:celery-worker]
command=python3 -m celery -A celery_app worker --loglevel=info --concurrency=2 --queues=cards,celery
directory=/Users/jose.silva.lb/LBPay/supercore/scripts/squad-orchestrator
autostart=true
autorestart=true
stdout_logfile=/tmp/celery_worker.log
stderr_logfile=/tmp/celery_worker.log
```

---

### 2. Monitoring Dashboard
Implementar dashboard web no portal frontend para visualizar:
- Lista de workers ativos
- Tasks em execu√ß√£o com progress bars
- Hist√≥rico de tasks (success/failure/retry)
- M√©tricas (throughput, latency, errors)

---

### 3. Task Priority
Implementar prioriza√ß√£o de tasks:
```python
# tasks.py
@celery_app.task(priority=9)  # 0-9, 9 = highest
def execute_critical_card_task(card_id: str):
    pass

# Usage
execute_critical_card_task.apply_async((card_id,), priority=9)
```

---

### 4. Task Chaining
Implementar workflows com chains e groups:
```python
from celery import chain, group

# Sequential execution
workflow = chain(
    execute_card_task.s('EPIC-001'),
    execute_card_task.s('PROD-001'),
    execute_card_task.s('PROD-002')
)
workflow.apply_async()

# Parallel execution
parallel_tasks = group(
    execute_card_task.s('PROD-001'),
    execute_card_task.s('PROD-002'),
    execute_card_task.s('PROD-003')
)
parallel_tasks.apply_async()
```

---

## üìù Testing

### Unit Tests
```python
# tests/test_tasks.py
import pytest
from tasks import execute_card_task
from celery.result import EagerResult

@pytest.fixture
def celery_eager_mode(celery_app):
    celery_app.conf.task_always_eager = True
    return celery_app

def test_execute_card_task_success(celery_eager_mode):
    result = execute_card_task.delay('PROD-001')
    assert result.status == 'SUCCESS'
    assert result.result['status'] == 'success'

def test_execute_card_task_failure(celery_eager_mode):
    # Card n√£o existe
    with pytest.raises(Exception):
        result = execute_card_task.delay('INVALID-CARD')
```

### Integration Tests
```bash
# Start test environment
docker-compose -f docker-compose.test.yml up -d

# Run tests
pytest tests/integration/test_celery_integration.py -v

# Cleanup
docker-compose -f docker-compose.test.yml down
```

---

## üìö Refer√™ncias

- [Celery Documentation](https://docs.celeryq.dev/en/stable/)
- [Redis Documentation](https://redis.io/docs/)
- [FastAPI WebSocket](https://fastapi.tiangolo.com/advanced/websockets/)
- [Celery Best Practices](https://docs.celeryq.dev/en/stable/userguide/tasks.html#best-practices)
- [Redis Pub/Sub](https://redis.io/docs/manual/pubsub/)

---

## ‚úÖ Status Final

**Integra√ß√£o Celery + Redis**: ‚úÖ **COMPLETA E TESTADA**

**Funcionalidades Implementadas**:
- ‚úÖ Celery app configuration
- ‚úÖ Custom ProgressReportingTask
- ‚úÖ execute_card_task with retry
- ‚úÖ Real-time progress reporting
- ‚úÖ Real-time logging via Redis
- ‚úÖ WebSocket streaming
- ‚úÖ Enhanced API endpoints
- ‚úÖ Backward compatibility with subprocess
- ‚úÖ Health checks
- ‚úÖ Database migration

**Tested**:
- ‚úÖ Worker startup
- ‚úÖ Task enqueuing
- ‚úÖ Task execution
- ‚úÖ Progress updates
- ‚úÖ Process monitoring
- ‚úÖ Timeout handling
- ‚úÖ Health endpoint

**Ready for Production**: ‚ö†Ô∏è  Aguardando supervisord configuration

---

**Documento criado**: 2025-12-22 11:15 BRT
**Autor**: Claude Sonnet 4.5
**Status**: ‚úÖ COMPLETO
