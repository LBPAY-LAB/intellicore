# FASE 2 - BRAIN: Architect Agent (Consci√™ncia Geradora)

> **"A plataforma n√£o apenas executa. Ela PENSA, APRENDE e CRIA."**

## üìã √çndice

1. [Vis√£o Geral](#vis√£o-geral)
2. [Objetivos da Fase 2](#objetivos-da-fase-2)
3. [Arquitetura do Architect Agent](#arquitetura-do-architect-agent)
4. [Componentes Principais](#componentes-principais)
5. [Fluxo de Gera√ß√£o Autom√°tica](#fluxo-de-gera√ß√£o-autom√°tica)
6. [Sprints de Implementa√ß√£o](#sprints-de-implementa√ß√£o)
7. [M√©tricas de Sucesso](#m√©tricas-de-sucesso)
8. [Exemplos Concretos](#exemplos-concretos)

---

## üéØ Vis√£o Geral

### O Que √© a Fase 2?

A Fase 2 transforma a plataforma SuperCore de um **executor de objetos** em um **criador de objetos**.

**Fase 1** (Foundation):
```
Humano ‚Üí Assistente ‚Üí object_definition ‚Üí Inst√¢ncias
```

**Fase 2** (Brain):
```
Documento BACEN ‚Üí Architect Agent ‚Üí object_definitions + validation_rules + FSMs
         ‚Üì
    M√≥dulo Completo Gerado Automaticamente
```

### Por Que √© Revolucion√°rio?

1. **Time de Produto/Compliance n√£o precisa mais descrever objetos** - apenas fornece PDFs
2. **Architect Agent l√™, interpreta e implementa** - igual a um desenvolvedor s√™nior
3. **Gera√ß√£o de m√≥dulos completos** - PIX, TED, KYC, Limites, tudo em dias
4. **Versionamento autom√°tico** - Agent detecta mudan√ßas em normas e atualiza
5. **Compliance sempre atualizado** - Agent monitora site do BACEN

---

## üéØ Objetivos da Fase 2

### Objetivos T√©cnicos

1. ‚úÖ **Document Intelligence System**
   - Parser de PDFs (Circulares, Resolu√ß√µes, Manuais BACEN)
   - Extra√ß√£o de estruturas (tabelas, listas, regras)
   - OCR para documentos escaneados
   - Detec√ß√£o de mudan√ßas em documentos versionados

2. ‚úÖ **Architect Agent Core**
   - LLM especializado em modelagem de dados
   - Gerador de JSON Schema a partir de texto normativo
   - Gerador de FSM a partir de fluxos descritos
   - Gerador de validation_rules a partir de regras BACEN
   - Gerador de UI hints automaticamente

3. ‚úÖ **Knowledge Base (Vector Store)**
   - Embeddings de toda documenta√ß√£o BACEN
   - Semantic search para contexto
   - Cross-referencing entre normas
   - Timeline de mudan√ßas regulat√≥rias

4. ‚úÖ **Validation & Testing**
   - Testes unit√°rios gerados automaticamente
   - Valida√ß√£o contra schemas existentes
   - Detec√ß√£o de conflitos entre normas
   - Rollback autom√°tico se valida√ß√£o falhar

5. ‚úÖ **Monitoring & Alerting**
   - Crawler do site BACEN para novas publica√ß√µes
   - Notifica√ß√£o de mudan√ßas em normas vigentes
   - Dashboard de compliance status
   - Auditoria de gera√ß√µes autom√°ticas

### Objetivos de Neg√≥cio

1. **Reduzir tempo de implementa√ß√£o de m√≥dulos de 3 meses para 3 dias**
2. **Eliminar 90% do trabalho manual de modelagem**
3. **Garantir compliance 100% com regulamenta√ß√µes**
4. **Permitir updates instant√¢neos quando BACEN muda normas**
5. **Documenta√ß√£o autom√°tica de todo m√≥dulo gerado**

---

## üèóÔ∏è Arquitetura do Architect Agent

### Vis√£o de Alto N√≠vel

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FASE 2: BRAIN LAYER                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  INPUT LAYER: Document Sources                                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  1. BACEN Documents                                             ‚îÇ
‚îÇ     ‚îú‚îÄ Circulares (ex: 3.978 - PLD/FT)                         ‚îÇ
‚îÇ     ‚îú‚îÄ Resolu√ß√µes (ex: 4.753 - KYC)                            ‚îÇ
‚îÇ     ‚îú‚îÄ Manuais (ex: Manual PIX)                                ‚îÇ
‚îÇ     ‚îî‚îÄ Comunicados                                              ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  2. Internal Docs                                               ‚îÇ
‚îÇ     ‚îú‚îÄ Pol√≠ticas internas                                       ‚îÇ
‚îÇ     ‚îú‚îÄ Regras de neg√≥cio                                        ‚îÇ
‚îÇ     ‚îî‚îÄ Requisitos de produto                                    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  3. API Specifications                                          ‚îÇ
‚îÇ     ‚îú‚îÄ Swagger/OpenAPI                                          ‚îÇ
‚îÇ     ‚îú‚îÄ GraphQL schemas                                          ‚îÇ
‚îÇ     ‚îî‚îÄ gRPC protos                                              ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PROCESSING LAYER: Architect Agent Pipeline                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ 1. DOCUMENT INGESTION                                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ PDF Parser (PyMuPDF, pdfplumber)                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ OCR (Tesseract, AWS Textract)                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ Table Extraction (Camelot, Tabula)              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îî‚îÄ Structure Detection (layouts, sections)         ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                       ‚Üì                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ 2. SEMANTIC ANALYSIS                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ Chunking (semantic splitting)                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ Embedding (OpenAI text-embedding-3-large)       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ Vector Store (pgvector)                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îî‚îÄ Entity Extraction (NER for objects, fields)     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                       ‚Üì                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ 3. SCHEMA GENERATION                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ LLM Prompt Engineering (Claude Opus 4)          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ JSON Schema Generator                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ FSM Generator (states + transitions)            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ Validation Rules Mapper                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îî‚îÄ UI Hints Generator                              ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                       ‚Üì                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ 4. VALIDATION & REFINEMENT                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ JSON Schema Validator                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ FSM Validator (no orphan states)                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ Conflict Detection (vs existing objects)        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ Test Generation (unit tests)                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îî‚îÄ Human Review Queue (ambiguities)                ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                       ‚Üì                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ 5. DEPLOYMENT                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ object_definition insertion                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ validation_rules creation                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ Migration generation (if schema changes)        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ Documentation generation                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚îî‚îÄ Notification (Slack, Email)                     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  OUTPUT LAYER: Generated Artifacts                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  1. Database Objects                                            ‚îÇ
‚îÇ     ‚îú‚îÄ object_definitions (JSON Schema, FSM, rules)            ‚îÇ
‚îÇ     ‚îú‚îÄ validation_rules (regex, functions, API calls)          ‚îÇ
‚îÇ     ‚îî‚îÄ Migrations (if needed)                                   ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  2. Documentation                                               ‚îÇ
‚îÇ     ‚îú‚îÄ README.md (m√≥dulo overview)                             ‚îÇ
‚îÇ     ‚îú‚îÄ API_SPEC.md (endpoints gerados)                         ‚îÇ
‚îÇ     ‚îú‚îÄ COMPLIANCE.md (normas BACEN referenciadas)             ‚îÇ
‚îÇ     ‚îî‚îÄ CHANGELOG.md (hist√≥rico de vers√µes)                     ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  3. Tests                                                       ‚îÇ
‚îÇ     ‚îú‚îÄ Unit tests (Go)                                         ‚îÇ
‚îÇ     ‚îú‚îÄ Integration tests                                       ‚îÇ
‚îÇ     ‚îî‚îÄ E2E tests (Playwright)                                  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  4. UI Components                                               ‚îÇ
‚îÇ     ‚îú‚îÄ Dynamic forms (gerados automaticamente)                 ‚îÇ
‚îÇ     ‚îú‚îÄ Dashboards (para visualiza√ß√£o)                          ‚îÇ
‚îÇ     ‚îî‚îÄ Reports (compliance reports)                            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üß© Componentes Principais

### 1. Document Intelligence Engine

**Responsabilidade**: Transformar documentos n√£o estruturados em dados estruturados.

**Stack Tecnol√≥gico**:
- **PyMuPDF (fitz)** - Parser PDF de alta performance
- **pdfplumber** - Extra√ß√£o de tabelas e texto
- **Tesseract OCR** - Para PDFs escaneados
- **AWS Textract** (opcional) - OCR avan√ßado com ML
- **Camelot** - Extra√ß√£o precisa de tabelas
- **spaCy** - NER (Named Entity Recognition)

**Arquitetura**:

```python
# document_intelligence/parser.py

from dataclasses import dataclass
from typing import List, Dict, Any
import fitz  # PyMuPDF
import pdfplumber
from pydantic import BaseModel

@dataclass
class DocumentSection:
    """Se√ß√£o extra√≠da do documento"""
    title: str
    level: int  # 1=Cap√≠tulo, 2=Se√ß√£o, 3=Subse√ß√£o
    content: str
    page_start: int
    page_end: int
    tables: List[Dict[str, Any]]
    lists: List[List[str]]
    metadata: Dict[str, Any]

class BACENDocumentParser:
    """Parser especializado em documentos BACEN"""

    def __init__(self):
        self.pdf_parser = None
        self.ocr_enabled = True
        self.table_extractor = None

    def parse(self, pdf_path: str) -> DocumentStructure:
        """
        Parse completo de documento BACEN

        Passos:
        1. Extra√ß√£o de texto (OCR se necess√°rio)
        2. Detec√ß√£o de estrutura (cap√≠tulos, se√ß√µes)
        3. Extra√ß√£o de tabelas
        4. Identifica√ß√£o de listas (enumera√ß√µes, requisitos)
        5. Extra√ß√£o de metadados (n√∫mero da norma, data, vig√™ncia)
        """

        doc = fitz.open(pdf_path)

        # 1. EXTRA√á√ÉO DE METADADOS
        metadata = self._extract_metadata(doc)
        # Ex: {"norma": "Circular 3.978", "data": "2020-01-23", "vigencia": "2020-03-01"}

        # 2. EXTRA√á√ÉO DE ESTRUTURA
        sections = self._extract_sections(doc)

        # 3. EXTRA√á√ÉO DE TABELAS
        with pdfplumber.open(pdf_path) as pdf:
            for i, page in enumerate(pdf.pages):
                tables = page.extract_tables()
                sections[i].tables.extend(tables)

        # 4. DETEC√á√ÉO DE LISTAS E ENUMERA√á√ïES
        for section in sections:
            section.lists = self._extract_lists(section.content)

        # 5. ENTITY EXTRACTION (objetos, campos, regras)
        entities = self._extract_entities(sections)

        return DocumentStructure(
            metadata=metadata,
            sections=sections,
            entities=entities,
            raw_text=self._get_full_text(doc)
        )

    def _extract_metadata(self, doc: fitz.Document) -> Dict[str, Any]:
        """Extrai metadados do documento"""

        # REGEX patterns para documentos BACEN
        patterns = {
            'circular': r'Circular\s+n¬∫?\s*(\d{1,5})',
            'resolucao': r'Resolu√ß√£o\s+n¬∫?\s*(\d{1,5})',
            'data': r'(\d{2}/\d{2}/\d{4})',
            'vigencia': r'vig√™ncia.*?(\d{2}/\d{2}/\d{4})'
        }

        first_page = doc[0].get_text()

        metadata = {}
        for key, pattern in patterns.items():
            match = re.search(pattern, first_page, re.IGNORECASE)
            if match:
                metadata[key] = match.group(1)

        return metadata

    def _extract_sections(self, doc: fitz.Document) -> List[DocumentSection]:
        """Detecta estrutura hier√°rquica do documento"""

        sections = []
        current_section = None

        for page_num, page in enumerate(doc):
            blocks = page.get_text("dict")["blocks"]

            for block in blocks:
                if "lines" not in block:
                    continue

                for line in block["lines"]:
                    text = " ".join([span["text"] for span in line["spans"]])

                    # Detecta t√≠tulos por tamanho de fonte e formato
                    font_size = line["spans"][0]["size"]
                    is_bold = "Bold" in line["spans"][0]["font"]
                    is_uppercase = text.isupper()

                    if font_size > 12 or (is_bold and is_uppercase):
                        # Novo t√≠tulo detectado
                        if current_section:
                            sections.append(current_section)

                        current_section = DocumentSection(
                            title=text,
                            level=self._infer_level(font_size, is_bold),
                            content="",
                            page_start=page_num,
                            page_end=page_num,
                            tables=[],
                            lists=[],
                            metadata={}
                        )
                    elif current_section:
                        current_section.content += text + "\n"
                        current_section.page_end = page_num

        if current_section:
            sections.append(current_section)

        return sections

    def _extract_entities(self, sections: List[DocumentSection]) -> Dict[str, List[str]]:
        """
        Extrai entidades mencionadas no documento usando NER

        Entidades alvo:
        - OBJECT_TYPE: "Cliente", "Conta", "Transa√ß√£o"
        - FIELD: "CPF", "Valor", "Data"
        - RULE: "limite", "valida√ß√£o", "prazo"
        - STATE: "ATIVO", "PENDENTE", "CANCELADO"
        """

        import spacy
        nlp = spacy.load("pt_core_news_lg")

        entities = {
            'objects': [],
            'fields': [],
            'rules': [],
            'states': [],
            'amounts': [],
            'dates': []
        }

        for section in sections:
            doc = nlp(section.content)

            # Entidades nomeadas
            for ent in doc.ents:
                if ent.label_ == "PER":  # Pode ser tipo de cliente
                    entities['objects'].append(ent.text)
                elif ent.label_ == "MONEY":
                    entities['amounts'].append(ent.text)
                elif ent.label_ == "DATE":
                    entities['dates'].append(ent.text)

            # Pattern matching para campos
            field_patterns = [
                r'campo\s+["\']?(\w+)["\']?',
                r'informa√ß√£o\s+de\s+(\w+)',
                r'dado\s+["\']?(\w+)["\']?'
            ]

            for pattern in field_patterns:
                matches = re.findall(pattern, section.content, re.IGNORECASE)
                entities['fields'].extend(matches)

            # Pattern matching para regras
            rule_patterns = [
                r'deve\s+(?:ser|conter|ter)\s+(.+?)(?:\.|,|;)',
                r'√©\s+(?:obrigat√≥rio|vedado|permitido)\s+(.+?)(?:\.|,|;)',
                r'limite\s+de\s+(.+?)(?:\.|,|;)'
            ]

            for pattern in rule_patterns:
                matches = re.findall(pattern, section.content, re.IGNORECASE)
                entities['rules'].extend(matches)

        # Deduplica e limpa
        for key in entities:
            entities[key] = list(set([e.strip() for e in entities[key] if e.strip()]))

        return entities
```

**Output Exemplo**:

```json
{
  "metadata": {
    "norma": "Circular 3.978",
    "titulo": "Preven√ß√£o √† Lavagem de Dinheiro (PLD) e Financiamento do Terrorismo (FT)",
    "data_publicacao": "2020-01-23",
    "vigencia_inicio": "2020-03-01",
    "orgao": "BACEN",
    "tipo": "Circular"
  },
  "sections": [
    {
      "title": "CAP√çTULO I - DISPOSI√á√ïES GERAIS",
      "level": 1,
      "content": "Art. 1¬∫ Esta Circular disp√µe sobre as pol√≠ticas, procedimentos...",
      "page_start": 1,
      "page_end": 3,
      "tables": [],
      "lists": [
        ["identifica√ß√£o do cliente", "cadastro atualizado", "an√°lise de risco"]
      ]
    },
    {
      "title": "CAP√çTULO II - LIMITES TRANSACIONAIS",
      "level": 1,
      "content": "Art. 5¬∫ As institui√ß√µes devem estabelecer limites...",
      "page_start": 4,
      "page_end": 6,
      "tables": [
        [
          ["Tipo Transa√ß√£o", "Limite Di√°rio", "Limite Noturno"],
          ["PIX", "R$ 20.000,00", "R$ 1.000,00"],
          ["TED", "Sem limite", "Sem limite"],
          ["Saque", "R$ 5.000,00", "N√£o permitido"]
        ]
      ],
      "lists": []
    }
  ],
  "entities": {
    "objects": ["Cliente", "Transa√ß√£o", "Conta"],
    "fields": ["CPF", "Nome", "Valor", "Data", "Tipo"],
    "rules": [
      "limite di√°rio de R$ 20.000,00 para PIX",
      "limite noturno de R$ 1.000,00 entre 20h e 6h",
      "valida√ß√£o de CPF obrigat√≥ria"
    ],
    "states": ["PENDENTE", "APROVADO", "REJEITADO"],
    "amounts": ["R$ 20.000,00", "R$ 1.000,00", "R$ 5.000,00"],
    "dates": ["2020-03-01"]
  }
}
```

### 2. Schema Generation Engine

**Responsabilidade**: Transformar documento estruturado em object_definition v√°lido.

**Arquitetura**:

```python
# architect_agent/schema_generator.py

from typing import Dict, Any, List
from pydantic import BaseModel
import anthropic
import json

class SchemaGenerationPrompt(BaseModel):
    """Template de prompt para gera√ß√£o de schema"""
    document_section: str
    entities_extracted: Dict[str, List[str]]
    context: str  # Contexto de outros objetos j√° definidos
    requirements: List[str]  # Requisitos espec√≠ficos

class SchemaGenerator:
    """Gera JSON Schema + FSM + Validation Rules a partir de documentos"""

    def __init__(self, llm_client: anthropic.Anthropic):
        self.llm = llm_client
        self.system_prompt = self._load_system_prompt()

    def generate_object_definition(
        self,
        document: DocumentStructure,
        target_object: str
    ) -> Dict[str, Any]:
        """
        Gera object_definition completo a partir de documento

        Args:
            document: Documento parseado (ex: Circular BACEN)
            target_object: Nome do objeto a ser gerado (ex: "transacao_pix")

        Returns:
            object_definition completo com schema, FSM, rules, UI hints
        """

        # 1. EXTRA√á√ÉO DE CONTEXTO RELEVANTE
        relevant_sections = self._find_relevant_sections(document, target_object)

        # 2. BUSCA DE OBJETOS RELACIONADOS (RAG)
        related_objects = self._find_related_objects(target_object)

        # 3. GERA√á√ÉO DE JSON SCHEMA
        schema = self._generate_json_schema(
            sections=relevant_sections,
            entities=document.entities,
            related_objects=related_objects
        )

        # 4. GERA√á√ÉO DE FSM
        fsm = self._generate_fsm(relevant_sections)

        # 5. GERA√á√ÉO DE VALIDATION RULES
        rules = self._generate_validation_rules(relevant_sections, schema)

        # 6. GERA√á√ÉO DE UI HINTS
        ui_hints = self._generate_ui_hints(schema)

        # 7. GERA√á√ÉO DE RELACIONAMENTOS PERMITIDOS
        relationships = self._generate_relationships(
            target_object,
            related_objects,
            document.entities
        )

        return {
            "name": self._slugify(target_object),
            "display_name": target_object.title(),
            "description": self._generate_description(relevant_sections),
            "schema": schema,
            "states": fsm,
            "rules": rules,
            "ui_hints": ui_hints,
            "relationships": relationships,
            "metadata": {
                "source_document": document.metadata.get("norma"),
                "generated_at": datetime.now().isoformat(),
                "generator_version": "2.0.0",
                "review_status": "PENDING"  # Requer revis√£o humana
            }
        }

    def _generate_json_schema(
        self,
        sections: List[DocumentSection],
        entities: Dict[str, List[str]],
        related_objects: List[Dict]
    ) -> Dict[str, Any]:
        """Gera JSON Schema usando LLM"""

        # Monta contexto para o LLM
        context = self._build_context(sections, entities, related_objects)

        prompt = f"""Voc√™ √© um especialista em modelagem de dados para Core Banking e JSON Schema Draft 7.

TAREFA: Gerar JSON Schema completo para um objeto de neg√≥cio baseado em documenta√ß√£o regulat√≥ria BACEN.

CONTEXTO DO DOCUMENTO:
{context}

CAMPOS IDENTIFICADOS:
{json.dumps(entities['fields'], indent=2, ensure_ascii=False)}

REGRAS IDENTIFICADAS:
{json.dumps(entities['rules'], indent=2, ensure_ascii=False)}

OBJETOS RELACIONADOS J√Å EXISTENTES:
{json.dumps([obj['name'] for obj in related_objects], indent=2)}

INSTRU√á√ïES:
1. Crie um JSON Schema Draft 7 v√°lido
2. Use tipos apropriados (string, number, boolean, object, array)
3. Defina "required" para campos obrigat√≥rios
4. Use "pattern" para valida√ß√µes regex (CPF, CNPJ, email, etc)
5. Use "enum" para campos de sele√ß√£o
6. Use "minimum", "maximum" para limites num√©ricos
7. Use "format" (date, date-time, email, uri) quando apropriado
8. Use "description" em portugu√™s para cada campo
9. Se um campo referencia outro objeto, use:
   {{
     "type": "string",
     "format": "uuid",
     "x-relationship": {{
       "target_object": "nome_do_objeto",
       "relationship_type": "TIPO_RELACAO"
     }}
   }}
10. Para valores monet√°rios, use:
    {{
      "type": "integer",
      "description": "Valor em centavos (ex: 10000 = R$ 100,00)",
      "minimum": 0
    }}

RETORNE APENAS O JSON SCHEMA V√ÅLIDO, SEM EXPLICA√á√ïES.
"""

        response = self.llm.messages.create(
            model="claude-opus-4-20250514",
            max_tokens=4000,
            temperature=0.1,  # Baixa temperatura para output determin√≠stico
            system=self.system_prompt,
            messages=[{"role": "user", "content": prompt}]
        )

        schema_json = response.content[0].text

        # Parse e valida
        try:
            schema = json.loads(schema_json)
            self._validate_json_schema(schema)
            return schema
        except Exception as e:
            raise ValueError(f"Schema inv√°lido gerado: {e}")

    def _generate_fsm(self, sections: List[DocumentSection]) -> Dict[str, Any]:
        """Gera Finite State Machine a partir de descri√ß√µes de fluxo"""

        # Busca se√ß√µes que mencionam estados, fluxos, processos
        flow_sections = [
            s for s in sections
            if any(keyword in s.content.lower() for keyword in [
                'estado', 'fluxo', 'processo', 'etapa', 'fase',
                'aprova√ß√£o', 'an√°lise', 'conclus√£o'
            ])
        ]

        if not flow_sections:
            # FSM padr√£o se n√£o houver informa√ß√£o
            return {
                "initial": "ATIVO",
                "states": ["ATIVO", "INATIVO"],
                "transitions": [
                    {
                        "from": "ATIVO",
                        "to": "INATIVO",
                        "event": "inativar",
                        "conditions": []
                    }
                ]
            }

        context = "\n\n".join([s.content for s in flow_sections])

        prompt = f"""Voc√™ √© um especialista em Finite State Machines (FSM) e processos de neg√≥cio.

TAREFA: Extrair estados e transi√ß√µes de um processo descrito em texto.

DESCRI√á√ÉO DO PROCESSO:
{context}

INSTRU√á√ïES:
1. Identifique todos os ESTADOS mencionados (ex: PENDENTE, APROVADO, REJEITADO)
2. Identifique o ESTADO INICIAL
3. Identifique todas as TRANSI√á√ïES poss√≠veis entre estados
4. Para cada transi√ß√£o, identifique:
   - Estado de origem (from)
   - Estado de destino (to)
   - Evento que dispara (event)
   - Condi√ß√µes necess√°rias (conditions, se houver)
5. Use nomes em UPPER_SNAKE_CASE para estados
6. Use nomes em snake_case para eventos

FORMATO DE OUTPUT (JSON):
{{
  "initial": "ESTADO_INICIAL",
  "states": ["ESTADO_1", "ESTADO_2", ...],
  "transitions": [
    {{
      "from": "ESTADO_1",
      "to": "ESTADO_2",
      "event": "nome_evento",
      "conditions": [
        {{
          "field": "campo",
          "operator": "==|!=|>|<|>=|<=",
          "value": "valor"
        }}
      ]
    }}
  ]
}}

RETORNE APENAS O JSON V√ÅLIDO, SEM EXPLICA√á√ïES.
"""

        response = self.llm.messages.create(
            model="claude-opus-4-20250514",
            max_tokens=2000,
            temperature=0.1,
            system=self.system_prompt,
            messages=[{"role": "user", "content": prompt}]
        )

        fsm_json = response.content[0].text

        try:
            fsm = json.loads(fsm_json)
            self._validate_fsm(fsm)
            return fsm
        except Exception as e:
            raise ValueError(f"FSM inv√°lido gerado: {e}")

    def _generate_validation_rules(
        self,
        sections: List[DocumentSection],
        schema: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Gera validation_rules a partir de regras mencionadas no documento"""

        # Extrai regras mencionadas
        rule_texts = []
        for section in sections:
            # Pattern matching para regras
            patterns = [
                r'deve\s+(.+?)(?:\.|;)',
                r'√©\s+obrigat√≥rio\s+(.+?)(?:\.|;)',
                r'n√£o\s+pode\s+(.+?)(?:\.|;)',
                r'limite\s+de\s+(.+?)(?:\.|;)',
                r'valida√ß√£o\s+de\s+(.+?)(?:\.|;)'
            ]

            for pattern in patterns:
                matches = re.findall(pattern, section.content, re.IGNORECASE)
                rule_texts.extend(matches)

        if not rule_texts:
            return []

        prompt = f"""Voc√™ √© um especialista em valida√ß√£o de dados e regras de neg√≥cio.

TAREFA: Converter regras de neg√≥cio em validation_rules execut√°veis.

JSON SCHEMA DO OBJETO:
{json.dumps(schema, indent=2, ensure_ascii=False)}

REGRAS MENCIONADAS NO DOCUMENTO:
{json.dumps(rule_texts, indent=2, ensure_ascii=False)}

TIPOS DE VALIDATION_RULES DISPON√çVEIS:
1. regex: {{
     "type": "regex",
     "field": "campo",
     "pattern": "regex_pattern",
     "error_message": "mensagem"
   }}

2. range: {{
     "type": "range",
     "field": "campo",
     "min": valor_minimo,
     "max": valor_maximo,
     "error_message": "mensagem"
   }}

3. api_call: {{
     "type": "api_call",
     "endpoint": "/api/validate/something",
     "method": "POST",
     "error_message": "mensagem"
   }}

4. function: {{
     "type": "function",
     "code": "c√≥digo_javascript",
     "error_message": "mensagem"
   }}

5. required_if: {{
     "type": "required_if",
     "field": "campo_alvo",
     "condition": {{
       "field": "campo_condicao",
       "operator": "==",
       "value": "valor"
     }},
     "error_message": "mensagem"
   }}

INSTRU√á√ïES:
1. Para cada regra, crie uma validation_rule execut√°vel
2. Use o tipo mais apropriado
3. Seja espec√≠fico nos patterns e condi√ß√µes
4. Mensagens de erro devem ser claras em portugu√™s
5. Se a regra referenciar outra tabela/API, use api_call

RETORNE ARRAY DE VALIDATION_RULES EM JSON, SEM EXPLICA√á√ïES.
"""

        response = self.llm.messages.create(
            model="claude-opus-4-20250514",
            max_tokens=3000,
            temperature=0.1,
            system=self.system_prompt,
            messages=[{"role": "user", "content": prompt}]
        )

        rules_json = response.content[0].text

        try:
            rules = json.loads(rules_json)
            return rules
        except Exception as e:
            raise ValueError(f"Validation rules inv√°lidas: {e}")

    def _generate_ui_hints(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        """Gera UI hints baseado no schema"""

        ui_hints = {
            "widgets": {},
            "labels": {},
            "help_text": {},
            "groups": []
        }

        for field_name, field_schema in schema.get("properties", {}).items():
            field_type = field_schema.get("type")
            field_format = field_schema.get("format")
            field_pattern = field_schema.get("pattern")

            # Inferir widget apropriado
            if field_pattern == r"^\d{11}$":
                ui_hints["widgets"][field_name] = "cpf"
            elif field_pattern == r"^\d{14}$":
                ui_hints["widgets"][field_name] = "cnpj"
            elif field_format == "date":
                ui_hints["widgets"][field_name] = "date"
            elif field_format == "date-time":
                ui_hints["widgets"][field_name] = "datetime"
            elif field_format == "email":
                ui_hints["widgets"][field_name] = "email"
            elif "enum" in field_schema:
                ui_hints["widgets"][field_name] = "select"
            elif field_type == "integer" and "valor" in field_name.lower():
                ui_hints["widgets"][field_name] = "currency"
            elif field_type == "string" and field_schema.get("maxLength", 0) > 200:
                ui_hints["widgets"][field_name] = "textarea"
            else:
                ui_hints["widgets"][field_name] = "text"

            # Label e help text
            ui_hints["labels"][field_name] = field_schema.get("title", field_name.replace("_", " ").title())
            ui_hints["help_text"][field_name] = field_schema.get("description", "")

        return ui_hints

    def _validate_json_schema(self, schema: Dict[str, Any]):
        """Valida se o schema gerado √© JSON Schema Draft 7 v√°lido"""
        from jsonschema import Draft7Validator, ValidationError

        try:
            Draft7Validator.check_schema(schema)
        except ValidationError as e:
            raise ValueError(f"JSON Schema inv√°lido: {e}")

    def _validate_fsm(self, fsm: Dict[str, Any]):
        """Valida se o FSM √© v√°lido (sem estados √≥rf√£os, etc)"""

        initial = fsm.get("initial")
        states = set(fsm.get("states", []))
        transitions = fsm.get("transitions", [])

        # Check 1: Initial state existe em states
        if initial not in states:
            raise ValueError(f"Estado inicial '{initial}' n√£o est√° em states")

        # Check 2: Nenhum estado √≥rf√£o (sem transi√ß√£o de entrada ou sa√≠da)
        states_in_transitions = set()
        for t in transitions:
            states_in_transitions.add(t["from"])
            states_in_transitions.add(t["to"])

        orphan_states = states - states_in_transitions - {initial}
        if orphan_states:
            raise ValueError(f"Estados √≥rf√£os detectados: {orphan_states}")

        # Check 3: Transi√ß√µes referenciam apenas estados v√°lidos
        for t in transitions:
            if t["from"] not in states:
                raise ValueError(f"Transi√ß√£o referencia estado inv√°lido: {t['from']}")
            if t["to"] not in states:
                raise ValueError(f"Transi√ß√£o referencia estado inv√°lido: {t['to']}")
```

### 3. Knowledge Base & Vector Store

**Responsabilidade**: Armazenar embeddings de documentos para RAG context.

```python
# architect_agent/knowledge_base.py

from typing import List, Dict, Any
import openai
from dataclasses import dataclass

@dataclass
class DocumentChunk:
    """Chunk de documento com embedding"""
    document_id: str
    chunk_id: str
    content: str
    embedding: List[float]
    metadata: Dict[str, Any]
    page_number: int
    section_title: str

class KnowledgeBase:
    """Vector store para documenta√ß√£o BACEN e contexto"""

    def __init__(self, pg_conn, openai_client: openai.OpenAI):
        self.pg = pg_conn
        self.openai = openai_client
        self.embedding_model = "text-embedding-3-large"

    async def ingest_document(self, document: DocumentStructure):
        """
        Ingere documento na knowledge base

        Passos:
        1. Chunk sem√¢ntico (por se√ß√£o)
        2. Gera embeddings
        3. Armazena em pgvector
        """

        chunks = []

        for section in document.sections:
            # Chunking por se√ß√£o (ou subdivide se muito grande)
            section_chunks = self._chunk_section(section)

            for chunk_text in section_chunks:
                # Gera embedding
                embedding = await self._generate_embedding(chunk_text)

                chunk = DocumentChunk(
                    document_id=document.metadata.get("norma"),
                    chunk_id=f"{document.metadata.get('norma')}_{section.title}_{len(chunks)}",
                    content=chunk_text,
                    embedding=embedding,
                    metadata=document.metadata,
                    page_number=section.page_start,
                    section_title=section.title
                )

                chunks.append(chunk)

        # Insere no banco
        await self._insert_chunks(chunks)

    async def search(
        self,
        query: str,
        top_k: int = 5,
        filters: Dict[str, Any] = None
    ) -> List[DocumentChunk]:
        """Busca sem√¢ntica na knowledge base"""

        # Gera embedding da query
        query_embedding = await self._generate_embedding(query)

        # Busca por similaridade (cosine)
        sql = """
            SELECT
                document_id,
                chunk_id,
                content,
                metadata,
                page_number,
                section_title,
                1 - (embedding <=> $1::vector) as similarity
            FROM document_embeddings
            WHERE 1=1
        """

        params = [query_embedding]

        # Aplica filtros
        if filters:
            if "document_type" in filters:
                sql += " AND metadata->>'tipo' = $2"
                params.append(filters["document_type"])

            if "date_after" in filters:
                sql += " AND (metadata->>'data_publicacao')::date >= $3"
                params.append(filters["date_after"])

        sql += f" ORDER BY similarity DESC LIMIT {top_k}"

        results = await self.pg.fetch(sql, *params)

        return [
            DocumentChunk(
                document_id=r["document_id"],
                chunk_id=r["chunk_id"],
                content=r["content"],
                embedding=[],  # N√£o retorna embedding completo
                metadata=r["metadata"],
                page_number=r["page_number"],
                section_title=r["section_title"]
            )
            for r in results
        ]

    async def _generate_embedding(self, text: str) -> List[float]:
        """Gera embedding usando OpenAI"""

        response = self.openai.embeddings.create(
            model=self.embedding_model,
            input=text,
            dimensions=3072  # text-embedding-3-large
        )

        return response.data[0].embedding

    def _chunk_section(self, section: DocumentSection, max_size: int = 1000) -> List[str]:
        """Divide se√ß√£o em chunks se necess√°rio"""

        if len(section.content) <= max_size:
            return [section.content]

        # Divide por par√°grafos
        paragraphs = section.content.split("\n\n")

        chunks = []
        current_chunk = ""

        for para in paragraphs:
            if len(current_chunk) + len(para) <= max_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para + "\n\n"

        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks
```

---

## üîÑ Fluxo de Gera√ß√£o Autom√°tica

### Caso de Uso 1: Gerar M√≥dulo PIX Completo

**Input**: Manual PIX BACEN (PDF de 300 p√°ginas)

**Fluxo**:

```
1. UPLOAD DO DOCUMENTO
   ‚îî‚îÄ> POST /api/architect/documents/upload
       Body: {file: manual_pix.pdf}

2. PARSING (5-10 minutos)
   ‚îú‚îÄ> Document Intelligence Engine processa PDF
   ‚îú‚îÄ> Extrai 50 se√ß√µes, 30 tabelas, 200 regras
   ‚îî‚îÄ> Armazena estrutura + embeddings

3. ENTITY DETECTION
   ‚îú‚îÄ> Identifica objetos: TransacaoPix, ChavePix, DevolucaoPix
   ‚îú‚îÄ> Identifica campos: chave, valor, infoPagador, endToEndId
   ‚îî‚îÄ> Identifica regras: limite_noturno, validacao_chave

4. GERA√á√ÉO AUTOM√ÅTICA (por objeto)

   4.1. TransacaoPix
        ‚îú‚îÄ> Schema Generator cria JSON Schema
        ‚îÇ   ‚îî‚îÄ> 25 campos (chave, valor, timestamp, status, etc)
        ‚îú‚îÄ> FSM Generator cria m√°quina de estados
        ‚îÇ   ‚îî‚îÄ> 8 estados: INICIADA ‚Üí VALIDANDO ‚Üí LIQUIDADA ‚Üí ...
        ‚îú‚îÄ> Validation Rules Generator
        ‚îÇ   ‚îî‚îÄ> 15 regras (valor_minimo, chave_valida, limite_noturno, etc)
        ‚îî‚îÄ> UI Hints Generator
            ‚îî‚îÄ> Widgets para cada campo

   4.2. ChavePix
        ‚îú‚îÄ> 12 campos (tipo, valor, titular, etc)
        ‚îú‚îÄ> 4 estados (CRIADA, ATIVA, BLOQUEADA, EXCLUIDA)
        ‚îî‚îÄ> 8 regras de valida√ß√£o

   4.3. DevolucaoPix
        ‚îú‚îÄ> 10 campos
        ‚îú‚îÄ> 5 estados
        ‚îî‚îÄ> 6 regras

5. VALIDA√á√ÉO
   ‚îú‚îÄ> Valida JSON Schemas (Draft 7)
   ‚îú‚îÄ> Valida FSMs (sem √≥rf√£os)
   ‚îú‚îÄ> Detecta conflitos com objetos existentes
   ‚îî‚îÄ> Gera testes unit√°rios automaticamente

6. REVIEW QUEUE
   ‚îú‚îÄ> Notifica time de Compliance
   ‚îú‚îÄ> Dashboard mostra preview dos objetos gerados
   ‚îú‚îÄ> Permite ajustes manuais
   ‚îî‚îÄ> Aguarda aprova√ß√£o

7. DEPLOYMENT (ap√≥s aprova√ß√£o)
   ‚îú‚îÄ> INSERT em object_definitions (3 objetos)
   ‚îú‚îÄ> INSERT em validation_rules (29 regras)
   ‚îú‚îÄ> Gera migration SQL (se necess√°rio)
   ‚îú‚îÄ> Gera documenta√ß√£o autom√°tica
   ‚îú‚îÄ> Notifica time via Slack
   ‚îî‚îÄ> Frontend j√° renderiza formul√°rios automaticamente

8. TESTING AUTOM√ÅTICO
   ‚îú‚îÄ> Executa testes unit√°rios gerados
   ‚îú‚îÄ> Executa testes de valida√ß√£o
   ‚îú‚îÄ> Valida integra√ß√£o com frontend
   ‚îî‚îÄ> Relat√≥rio de cobertura

OUTPUT FINAL:
‚úÖ M√≥dulo PIX completo implementado em 30 minutos
‚úÖ 3 object_definitions
‚úÖ 29 validation_rules
‚úÖ 50+ testes unit√°rios
‚úÖ Documenta√ß√£o completa
‚úÖ UI funcionando automaticamente
```

### Caso de Uso 2: Atualizar Regra Existente

**Input**: Circular 4.XXX altera limite noturno PIX de R$ 1.000 para R$ 2.500

**Fluxo**:

```
1. BACEN CRAWLER detecta nova publica√ß√£o
   ‚îî‚îÄ> Webhook: nova_circular_publicada

2. DOWNLOAD & PARSING
   ‚îú‚îÄ> Download autom√°tico do PDF
   ‚îî‚îÄ> Parsing da circular

3. CHANGE DETECTION
   ‚îú‚îÄ> Compara com circular anterior
   ‚îú‚îÄ> Detecta: "limite noturno PIX alterado"
   ‚îî‚îÄ> Identifica validation_rule afetada: "limite_pix_noturno"

4. UPDATE AUTOM√ÅTICO (com aprova√ß√£o)
   ‚îú‚îÄ> Cria vers√£o 2 da validation_rule
   ‚îú‚îÄ> Mant√©m vers√£o 1 para hist√≥rico
   ‚îú‚îÄ> Agenda vig√™ncia para data especificada
   ‚îî‚îÄ> Notifica Compliance para aprova√ß√£o

5. DEPLOYMENT AGENDADO
   ‚îî‚îÄ> No dia da vig√™ncia, ativa automaticamente

6. AUDITORIA
   ‚îî‚îÄ> Registra mudan√ßa em audit_log com refer√™ncia √† circular
```

---

## üìÖ Sprints de Implementa√ß√£o (Fase 2 - 12 semanas)

### Sprint 7-8: Document Intelligence Engine (Semanas 1-2)

**Objetivos**:
- [ ] Parser PDF com PyMuPDF + pdfplumber
- [ ] OCR com Tesseract
- [ ] Table extraction com Camelot
- [ ] Structure detection (sections, headings)
- [ ] Entity extraction com spaCy
- [ ] API endpoint: POST /api/architect/documents/upload

**Entregas**:
- DocumentParser class completa
- Testes com 5 documentos BACEN reais
- Cobertura ‚â• 80%

### Sprint 9-10: Schema Generation Engine (Semanas 3-4)

**Objetivos**:
- [ ] SchemaGenerator com Claude Opus 4
- [ ] FSM Generator
- [ ] Validation Rules Generator
- [ ] UI Hints Generator
- [ ] Validation pipeline (schema + FSM)
- [ ] API endpoint: POST /api/architect/generate

**Entregas**:
- SchemaGenerator class completa
- Testes com 3 objetos diferentes
- Valida√ß√£o autom√°tica funcionando

### Sprint 11-12: Knowledge Base & Vector Store (Semanas 5-6)

**Objetivos**:
- [ ] Knowledge base schema (pgvector)
- [ ] Document chunking
- [ ] Embedding generation (OpenAI)
- [ ] Semantic search
- [ ] API endpoint: GET /api/architect/search

**Entregas**:
- KnowledgeBase class completa
- 10 documentos BACEN indexados
- Search latency < 200ms

### Sprint 13-14: Review & Deployment System (Semanas 7-8)

**Objetivos**:
- [ ] Review Queue UI (frontend)
- [ ] Preview de objetos gerados
- [ ] Edi√ß√£o manual (se necess√°rio)
- [ ] Approval workflow
- [ ] Deployment autom√°tico ap√≥s aprova√ß√£o
- [ ] Rollback mechanism

**Entregas**:
- Review dashboard completo
- Workflow de aprova√ß√£o funcionando
- Deployment testado

### Sprint 15-16: BACEN Crawler & Monitoring (Semanas 9-10)

**Objetivos**:
- [ ] Crawler do site BACEN
- [ ] Detec√ß√£o de novas publica√ß√µes
- [ ] Download autom√°tico
- [ ] Change detection (diff entre vers√µes)
- [ ] Alerting (Slack, email)
- [ ] Scheduler (cron jobs)

**Entregas**:
- Crawler funcionando (daily)
- Alerting configurado
- Dashboard de publica√ß√µes

### Sprint 17-18: Integration & Polish (Semanas 11-12)

**Objetivos**:
- [ ] Integra√ß√£o end-to-end testada
- [ ] Gera√ß√£o de M√≥dulo PIX completo (teste real)
- [ ] Performance optimization
- [ ] Documentation completa
- [ ] Monitoring dashboards
- [ ] User training

**Entregas**:
- M√≥dulo PIX gerado com sucesso
- Documenta√ß√£o completa
- Training materials
- Production-ready

---

## üìä M√©tricas de Sucesso

### KPIs T√©cnicos

| M√©trica | Objetivo | Medi√ß√£o |
|---------|----------|---------|
| **Document Parsing Accuracy** | ‚â• 95% | Compara√ß√£o manual vs autom√°tico |
| **Schema Generation Quality** | ‚â• 90% | Aprova√ß√£o em review |
| **Entity Extraction Precision** | ‚â• 85% | F1-score em dataset anotado |
| **Knowledge Base Search Relevance** | ‚â• 80% | NDCG@5 |
| **End-to-End Generation Time** | < 30 min | Tempo total (upload ‚Üí deployment) |
| **False Positive Rate (conflicts)** | < 5% | Conflitos detectados incorretamente |
| **Test Coverage (generated code)** | ‚â• 80% | Cobertura de testes gerados |

### KPIs de Neg√≥cio

| M√©trica | Objetivo | Impacto |
|---------|----------|---------|
| **Tempo de Implementa√ß√£o de M√≥dulo** | 3 dias (vs 3 meses) | 30x mais r√°pido |
| **Custo de Modelagem** | -90% | Elimina trabalho manual |
| **Compliance Updates** | < 24h | Autom√°tico ap√≥s publica√ß√£o BACEN |
| **Qualidade de Documenta√ß√£o** | 100% | Gerada automaticamente |
| **Erros de Implementa√ß√£o** | -70% | Valida√ß√£o autom√°tica |

---

## üéØ Exemplos Concretos

### Exemplo 1: Gera√ß√£o de TransacaoPix

**Input**: Manual PIX BACEN - Cap√≠tulo "Inicia√ß√£o de Transa√ß√£o"

**Document Section**:
```
Artigo 5¬∫ - A transa√ß√£o PIX deve conter os seguintes campos:

1. endToEndId: identificador √∫nico da transa√ß√£o (32 caracteres alfanum√©ricos)
2. valor: valor em reais, m√≠nimo de R$ 0,01
3. chave: chave PIX do destinat√°rio (CPF, CNPJ, email, telefone ou aleat√≥ria)
4. infoPagador: informa√ß√£o do pagador (at√© 140 caracteres, opcional)
5. timestamp: data e hora da inicia√ß√£o

Artigo 6¬∫ - Limites transacionais:
- Limite di√°rio: R$ 20.000,00 por usu√°rio
- Limite noturno (20h √†s 6h): R$ 1.000,00 por transa√ß√£o

Artigo 7¬∫ - Estados da transa√ß√£o:
INICIADA ‚Üí VALIDANDO ‚Üí APROVADA ‚Üí LIQUIDADA
         ‚Üì            ‚Üì
      REJEITADA    CANCELADA
```

**Generated object_definition**:

```json
{
  "name": "transacao_pix",
  "display_name": "Transa√ß√£o PIX",
  "description": "Transa√ß√£o de pagamento instant√¢neo via sistema PIX do Banco Central",
  "version": 1,
  "schema": {
    "type": "object",
    "properties": {
      "end_to_end_id": {
        "type": "string",
        "pattern": "^[A-Za-z0-9]{32}$",
        "description": "Identificador √∫nico da transa√ß√£o (E2EID)"
      },
      "valor": {
        "type": "integer",
        "minimum": 1,
        "description": "Valor em centavos (ex: 10000 = R$ 100,00)"
      },
      "chave": {
        "type": "string",
        "description": "Chave PIX do destinat√°rio"
      },
      "tipo_chave": {
        "type": "string",
        "enum": ["CPF", "CNPJ", "EMAIL", "TELEFONE", "ALEATORIA"]
      },
      "info_pagador": {
        "type": "string",
        "maxLength": 140,
        "description": "Informa√ß√£o adicional do pagador (opcional)"
      },
      "timestamp": {
        "type": "string",
        "format": "date-time",
        "description": "Data e hora da inicia√ß√£o da transa√ß√£o"
      },
      "pagador_id": {
        "type": "string",
        "format": "uuid",
        "x-relationship": {
          "target_object": "cliente_pf",
          "relationship_type": "PAGADOR_DE"
        }
      },
      "beneficiario_id": {
        "type": "string",
        "format": "uuid",
        "x-relationship": {
          "target_object": "cliente_pf",
          "relationship_type": "BENEFICIARIO_DE"
        }
      }
    },
    "required": ["end_to_end_id", "valor", "chave", "tipo_chave", "timestamp", "pagador_id"]
  },
  "states": {
    "initial": "INICIADA",
    "states": [
      "INICIADA",
      "VALIDANDO",
      "APROVADA",
      "LIQUIDADA",
      "REJEITADA",
      "CANCELADA"
    ],
    "transitions": [
      {
        "from": "INICIADA",
        "to": "VALIDANDO",
        "event": "validar",
        "conditions": []
      },
      {
        "from": "INICIADA",
        "to": "REJEITADA",
        "event": "rejeitar",
        "conditions": []
      },
      {
        "from": "VALIDANDO",
        "to": "APROVADA",
        "event": "aprovar",
        "conditions": [
          {
            "field": "valor",
            "operator": "<=",
            "value": 2000000,
            "error": "Valor excede limite di√°rio"
          }
        ]
      },
      {
        "from": "VALIDANDO",
        "to": "REJEITADA",
        "event": "rejeitar",
        "conditions": []
      },
      {
        "from": "APROVADA",
        "to": "LIQUIDADA",
        "event": "liquidar",
        "conditions": []
      },
      {
        "from": "APROVADA",
        "to": "CANCELADA",
        "event": "cancelar",
        "conditions": []
      }
    ]
  },
  "rules": [
    {
      "name": "valor_minimo",
      "type": "range",
      "field": "valor",
      "min": 1,
      "error_message": "Valor m√≠nimo: R$ 0,01"
    },
    {
      "name": "limite_diario",
      "type": "api_call",
      "endpoint": "/api/validate/limite-diario-pix",
      "method": "POST",
      "payload": {
        "pagador_id": "{{pagador_id}}",
        "valor": "{{valor}}"
      },
      "error_message": "Limite di√°rio de R$ 20.000,00 excedido"
    },
    {
      "name": "limite_noturno",
      "type": "function",
      "code": "const hora = new Date(data.timestamp).getHours(); if (hora >= 20 || hora < 6) { return data.valor <= 100000; } return true;",
      "error_message": "Limite noturno (20h-6h): R$ 1.000,00 por transa√ß√£o"
    },
    {
      "name": "chave_valida",
      "type": "api_call",
      "endpoint": "/api/bacen/dict/consultar-chave",
      "method": "GET",
      "error_message": "Chave PIX inv√°lida ou n√£o cadastrada"
    }
  ],
  "ui_hints": {
    "widgets": {
      "end_to_end_id": "text",
      "valor": "currency",
      "chave": "text",
      "tipo_chave": "select",
      "info_pagador": "textarea",
      "timestamp": "datetime",
      "pagador_id": "relationship",
      "beneficiario_id": "relationship"
    },
    "labels": {
      "end_to_end_id": "End-to-End ID",
      "valor": "Valor",
      "chave": "Chave PIX Destino",
      "tipo_chave": "Tipo de Chave",
      "info_pagador": "Informa√ß√£o do Pagador",
      "timestamp": "Data/Hora",
      "pagador_id": "Pagador",
      "beneficiario_id": "Benefici√°rio"
    },
    "help_text": {
      "end_to_end_id": "Gerado automaticamente pelo sistema. 32 caracteres alfanum√©ricos.",
      "valor": "Valor da transa√ß√£o em reais. M√≠nimo: R$ 0,01. Limite di√°rio: R$ 20.000,00.",
      "chave": "Chave PIX do destinat√°rio (CPF, CNPJ, email, telefone ou chave aleat√≥ria).",
      "info_pagador": "Mensagem opcional do pagador (at√© 140 caracteres)."
    },
    "groups": [
      {
        "title": "Dados da Transa√ß√£o",
        "fields": ["end_to_end_id", "valor", "timestamp"]
      },
      {
        "title": "Destinat√°rio",
        "fields": ["chave", "tipo_chave", "beneficiario_id"]
      },
      {
        "title": "Informa√ß√µes Adicionais",
        "fields": ["info_pagador"]
      }
    ]
  },
  "relationships": [
    "PAGADOR_DE",
    "BENEFICIARIO_DE"
  ],
  "metadata": {
    "source_document": "Manual PIX BACEN v3.1",
    "generated_at": "2024-01-15T10:30:00Z",
    "generator_version": "2.0.0",
    "review_status": "APPROVED",
    "approved_by": "compliance@lbpay.com.br",
    "approved_at": "2024-01-15T14:00:00Z"
  }
}
```

**Generated Tests** (auto):

```go
// backend/internal/handlers/transacao_pix_test.go
// AUTO-GENERATED by Architect Agent

func TestTransacaoPix_ValorMinimo(t *testing.T) {
    instance := &Instance{
        ObjectDefinitionID: uuid.MustParse("transacao-pix-id"),
        Data: map[string]interface{}{
            "end_to_end_id": "E12345678202401151030ABCD1234",
            "valor":         0, // Inv√°lido
            "chave":         "12345678901",
            "tipo_chave":    "CPF",
            "timestamp":     "2024-01-15T10:30:00Z",
            "pagador_id":    uuid.New().String(),
        },
    }

    err := validator.Validate(instance)
    assert.Error(t, err)
    assert.Contains(t, err.Error(), "Valor m√≠nimo: R$ 0,01")
}

func TestTransacaoPix_LimiteNoturno(t *testing.T) {
    instance := &Instance{
        ObjectDefinitionID: uuid.MustParse("transacao-pix-id"),
        Data: map[string]interface{}{
            "end_to_end_id": "E12345678202401152100ABCD1234",
            "valor":         150000, // R$ 1.500,00 √†s 21h - excede limite
            "chave":         "12345678901",
            "tipo_chave":    "CPF",
            "timestamp":     "2024-01-15T21:00:00Z", // Hor√°rio noturno
            "pagador_id":    uuid.New().String(),
        },
    }

    err := validator.Validate(instance)
    assert.Error(t, err)
    assert.Contains(t, err.Error(), "Limite noturno")
}

// ... mais 15 testes gerados automaticamente
```

---

## üöÄ Pr√≥ximos Passos

1. **Aprovar este documento** (FASE_2_ESCOPO_TECNICO_COMPLETO.md)
2. **Definir Stack de Implementa√ß√£o**:
   - Python 3.11+ para Document Intelligence
   - Go mant√©m backend core
   - PostgreSQL + pgvector para Knowledge Base
3. **Setup ambiente Python**:
   - PyMuPDF, pdfplumber, Tesseract
   - spaCy com modelo pt_core_news_lg
   - OpenAI SDK, Anthropic SDK
4. **Criar reposit√≥rio de documentos BACEN** (seed inicial)
5. **Iniciar Sprint 7**: Document Intelligence Engine

---

**Status**: üìù **ESPECIFICA√á√ÉO COMPLETA - AGUARDANDO APROVA√á√ÉO**

**Pr√≥xima Fase**: Implementa√ß√£o (12 semanas)

**Impacto Esperado**:
- ‚úÖ Redu√ß√£o de 90% no tempo de modelagem
- ‚úÖ Compliance 100% com regulamenta√ß√µes
- ‚úÖ Updates autom√°ticos quando BACEN muda normas
- ‚úÖ Documenta√ß√£o autom√°tica completa
- ‚úÖ M√≥dulos completos gerados em dias (vs meses)

---

*Documento criado por: Architect Team*
*Data: 2024-01-15*
*Vers√£o: 1.0*
