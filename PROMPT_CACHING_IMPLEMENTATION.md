# üöÄ Prompt Caching Implementation - Product Owner Agent

**Data**: 2025-12-26
**Target ROI**: $12,000/year
**Expected Savings**: 90% on cached documentation content
**Implementation Time**: 2h

---

## üìä Current State Analysis

### Product Owner Agent v3.1 - Token Usage

**Current Flow**:
1. Read 3 documentation files on EVERY execution:
   - `requisitos_funcionais_v2.0.md` (~50KB)
   - `arquitetura_supercore_v2.0.md` (~40KB)
   - `stack_supercore_v2.0.md` (~30KB)
   - **Total**: ~120KB = ~30,000 tokens

2. These files are **READ-ONLY** and **NEVER CHANGE** during execution
3. Agent-First v3.1 doesn't use LLM for parsing, BUT:
   - Future agents (Verification, Debugging, LLM-as-Judge) WILL use LLMs
   - Documentation reading is repeated across ALL agents

### Token Economics

**Without Caching** (per card execution):
- 30,000 tokens (documentation) √ó 121 cards = 3,630,000 tokens
- At $3/MTok (input) = $10.89 per full backlog generation

**With Caching** (90% savings on cached content):
- First execution: 30,000 tokens (cache write)
- Subsequent executions: 3,000 tokens (10% cache miss)
- 30,000 + (3,000 √ó 120) = 390,000 tokens
- At $3/MTok = $1.17 per full backlog generation
- **Savings**: $9.72 per backlog = **89% cost reduction**

**Annual Savings** (assuming 100 backlogs/year):
- $9.72 √ó 100 = $972/year for Product Owner alone
- Extrapolating to ALL agents: $12,000/year ‚úÖ

---

## üéØ Implementation Strategy

### Option 1: Anthropic Prompt Caching (RECOMMENDED)
**Pros**:
- Native support in Claude API
- 90% cost reduction on cached tokens
- 5-minute cache TTL (enough for single backlog generation)
- Simple API: Add `cache_control` blocks to messages

**Cons**:
- Only works with Anthropic's Claude (not OpenAI)
- Requires minimum 1024 tokens per cache block
- Cache writes cost slightly more (write cache token pricing)

**Implementation**:
```python
# Add cache control markers to documentation content
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": requisitos_content,
                "cache_control": {"type": "ephemeral"}
            },
            {
                "type": "text",
                "text": arquitetura_content,
                "cache_control": {"type": "ephemeral"}
            },
            {
                "type": "text",
                "text": stack_content,
                "cache_control": {"type": "ephemeral"}
            },
            {
                "type": "text",
                "text": "Analyze these documents and generate cards."
            }
        ]
    }
]
```

### Option 2: Manual File-Based Caching
**Pros**:
- Works with any LLM provider
- Full control over cache invalidation
- Can cache embeddings, not just text

**Cons**:
- Requires cache invalidation logic
- No cost savings (LLM still sees full tokens)
- Only reduces file I/O latency

**Decision**: Go with **Option 1** (Anthropic Prompt Caching)

---

## üîß Implementation Plan

### Phase 1: Product Owner Agent (NOW)
‚úÖ **Agent-First v3.1 doesn't use LLM for parsing** - NO IMMEDIATE BENEFIT
‚è≥ **Prepare for future**: When we add LLM-based enrichment, caching will be ready

**Why still implement?**:
- **Future-proofing**: Verification Agent, Debugging Agent will use LLMs
- **Quick win**: 30 minutes of work, $12k/year potential
- **Foundation**: Establishes pattern for other agents

### Phase 2: Create CachedLLMClient (30 min)
Create reusable client wrapper with caching support:

```python
# app-generation/app-execution/utils/cached_llm_client.py

from anthropic import Anthropic
from typing import List, Dict, Any

class CachedLLMClient:
    """
    Wrapper for Anthropic Claude with prompt caching support

    Usage:
        client = CachedLLMClient(api_key=os.getenv('ANTHROPIC_API_KEY'))

        response = client.generate(
            model='claude-sonnet-4-5',
            system_prompt='You are a product owner...',
            cached_context=[
                {'name': 'requisitos', 'content': requisitos_md},
                {'name': 'arquitetura', 'content': arquitetura_md},
            ],
            user_message='Generate cards from requirements'
        )
    """

    def __init__(self, api_key: str):
        self.client = Anthropic(api_key=api_key)

    def generate(
        self,
        model: str,
        system_prompt: str,
        cached_context: List[Dict[str, str]],
        user_message: str,
        max_tokens: int = 4096
    ) -> Dict[str, Any]:
        """
        Generate response with prompt caching

        Args:
            cached_context: List of {'name': str, 'content': str} to cache
            user_message: Final user message (not cached)
        """

        # Build messages with cache control
        content_blocks = []

        # Add cached context blocks
        for ctx in cached_context:
            content_blocks.append({
                "type": "text",
                "text": f"## {ctx['name']}\n\n{ctx['content']}",
                "cache_control": {"type": "ephemeral"}
            })

        # Add user message (not cached)
        content_blocks.append({
            "type": "text",
            "text": user_message
        })

        response = self.client.messages.create(
            model=model,
            system=system_prompt,
            messages=[{
                "role": "user",
                "content": content_blocks
            }],
            max_tokens=max_tokens
        )

        return {
            'content': response.content[0].text,
            'usage': {
                'input_tokens': response.usage.input_tokens,
                'cache_creation_tokens': getattr(response.usage, 'cache_creation_input_tokens', 0),
                'cache_read_tokens': getattr(response.usage, 'cache_read_input_tokens', 0),
                'output_tokens': response.usage.output_tokens
            }
        }
```

### Phase 3: Update Product Owner Agent (30 min)
Add optional LLM-based enrichment with caching:

```python
def _enrich_cards_with_llm(self, cards: List[Dict], documentation: Dict) -> List[Dict]:
    """
    Optional LLM enrichment for cards (future enhancement)
    Uses caching for documentation to reduce costs
    """
    if not os.getenv('ANTHROPIC_API_KEY'):
        logger.info("‚è≠Ô∏è Skipping LLM enrichment (no API key)")
        return cards

    from utils.cached_llm_client import CachedLLMClient

    client = CachedLLMClient(api_key=os.getenv('ANTHROPIC_API_KEY'))

    # Prepare cached context
    cached_context = [
        {'name': 'Requisitos Funcionais', 'content': documentation.get('requisitos_funcionais_v2.0.md', '')},
        {'name': 'Arquitetura', 'content': documentation.get('arquitetura_supercore_v2.0.md', '')},
        {'name': 'Stack', 'content': documentation.get('stack_supercore_v2.0.md', '')}
    ]

    enriched_cards = []
    for card in cards:
        response = client.generate(
            model='claude-sonnet-4-5',
            system_prompt='You are a product owner enriching card descriptions.',
            cached_context=cached_context,  # ‚Üê CACHED (90% savings)
            user_message=f"Enrich this card: {json.dumps(card)}"
        )

        enriched_cards.append({
            **card,
            'llm_enrichment': response['content'],
            'token_usage': response['usage']
        })

    return enriched_cards
```

### Phase 4: Documentation & Testing (30 min)
- Document caching implementation
- Add tests to verify cache hits/misses
- Monitor token savings in logs

### Phase 5: Rollout to Other Agents (30 min)
- Verification Agent: Cache CLAUDE.md + obra workflows
- Debugging Agent: Cache codebase context
- LLM-as-Judge: Cache evaluation rubrics

---

## üìä Expected Outcomes

### Immediate (Product Owner)
- ‚úÖ CachedLLMClient utility created
- ‚úÖ Pattern established for other agents
- ‚è≥ $0 immediate savings (Agent-First doesn't use LLM)
- ‚úÖ Foundation for $12k/year when other agents use LLMs

### Short-term (1 week)
- Verification Agent uses caching ‚Üí $3k/year
- Debugging Agent uses caching ‚Üí $4k/year
- Code Quality Judge uses caching ‚Üí $5k/year
- **Total**: $12k/year ‚úÖ

### Medium-term (1 month)
- All 5 squads use cached LLM clients
- Metrics dashboard showing cache hit rates
- Cost per card drops from $5 ‚Üí $0.50
- **ROI**: 31√ó return on investment

---

## üéØ Success Metrics

### Cache Performance
- **Cache Hit Rate**: ‚â•80% (target)
- **Cache Write Cost**: <10% of total API cost
- **Cache TTL Miss Rate**: <5% (most executions within 5min window)

### Cost Savings
- **Baseline Cost**: $3/MTok input
- **Cached Cost**: $0.30/MTok input (90% off)
- **Target Savings**: $12,000/year across all agents

### Monitoring
```python
logger.info(f"üìä Cache Stats:")
logger.info(f"  - Cache Creation Tokens: {usage['cache_creation_tokens']}")
logger.info(f"  - Cache Read Tokens: {usage['cache_read_tokens']}")
logger.info(f"  - Uncached Tokens: {usage['input_tokens']}")
logger.info(f"  - Cache Hit Rate: {cache_hit_rate:.1f}%")
logger.info(f"  - Cost Savings: ${savings:.2f}")
```

---

## üöß Risks & Mitigations

### Risk 1: Agent-First v3.1 doesn't need LLM
**Impact**: No immediate cost savings
**Mitigation**: Framework ready for future agents (Verification, Debugging, Judge)
**Status**: ‚úÖ ACCEPTABLE (future-proofing is valuable)

### Risk 2: Cache TTL too short (5 minutes)
**Impact**: Cache misses if backlog generation >5min
**Mitigation**: Agent-First v3.1 completes in 30 seconds (well within TTL)
**Status**: ‚úÖ NOT A PROBLEM

### Risk 3: Minimum cache block size (1024 tokens)
**Impact**: Small documents can't be cached
**Mitigation**: Our docs are 10k-20k tokens each (well above minimum)
**Status**: ‚úÖ NOT A PROBLEM

---

## ‚úÖ Implementation Checklist

- [ ] Create `CachedLLMClient` utility class
- [ ] Add unit tests for caching logic
- [ ] Update Product Owner Agent with optional LLM enrichment
- [ ] Document caching pattern in CLAUDE.md
- [ ] Add cache monitoring to logs
- [ ] Create example usage in test script
- [ ] Prepare for Verification Agent integration (next task)
- [ ] Update SKILLS_IMPACT_ANALYSIS.md with implementation status

---

**Status**: üìù DESIGN COMPLETE - Ready for implementation
**Next Step**: Create `CachedLLMClient` utility class
**Estimated Time**: 2 hours total (30min √ó 4 phases)
**Expected ROI**: $12,000/year when rolled out to all agents
