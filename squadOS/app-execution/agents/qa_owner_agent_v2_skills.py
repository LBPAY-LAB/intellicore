#!/usr/bin/env python3
"""
QA Owner Agent v2.0.0 - Skills-Only Architecture

Orchestrates comprehensive testing and validation using specialized skills:
- verification-agent: Evidence validation (obra ow-002)
- llm-judge: Code quality scoring
- debugging-agent: Systematic debugging (obra ow-006)

Key Differences from v1.0:
- âŒ v1.0: Direct test execution (agent-first)
- âœ… v2.0: Skills orchestration (delegator pattern)

Why Skills-Only (not Hybrid)?
- QA validates code, doesn't generate it
- No CLI scaffolding needed
- Direct delegation to validation skills

Pattern: 2 Phases (not 3 like backend/frontend)
- Phase 1 (20-60%): verification-agent (evidence validation)
- Phase 2 (60-80%): llm-judge (code quality scoring)
- Phase 3 (80-100%): debugging-agent (if needed)

ROI: Same $0 generation cost, but faster feedback loops and higher quality

Author: SquadOS Meta-Framework
Date: 2025-12-28
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional

# Import hybrid delegator
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))
from utils.hybrid_delegator import HybridDelegator

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class QAOwnerAgentV2:
    """
    QA Owner Agent v2.0 - Skills-Only Architecture

    Features:
    - Skills-only orchestration (no CLI scaffolding)
    - Checkpoint system for resumability
    - Progress reporting (7 stages)
    - Multi-layer validation (evidence, quality, debugging)
    - Zero-tolerance policy enforcement
    - Approval/rejection decision making
    """

    # Progress stages (7 total, simpler than backend/frontend's 9)
    STAGES = {
        'card_analyzed': 15,
        'artifacts_collected': 20,
        'evidence_validated': 60,       # Phase 1: verification-agent
        'quality_evaluated': 80,        # Phase 2: llm-judge
        'issues_debugged': 90,          # Phase 3: debugging-agent (if needed)
        'decision_made': 95,
        'completed': 100
    }

    # Zero-tolerance violations (from CLAUDE.md)
    ZERO_TOLERANCE_VIOLATIONS = [
        'mock_implementations',
        'todo_fixme_comments',
        'hardcoded_credentials',
        'missing_error_handling',
        'low_test_coverage',
        'critical_vulnerabilities',
        'stack_violations',
        'placeholder_data'
    ]

    # Thresholds
    THRESHOLDS = {
        'coverage_minimum': 80,          # Minimum test coverage %
        'quality_score_minimum': 8.0,    # Minimum LLM-as-Judge score (0-10)
        'max_debugging_attempts': 3      # Max debugging cycles before escalation
    }

    def __init__(self):
        """Initialize QA Owner Agent v2.0"""
        # Paths
        self.base_dir = Path(__file__).parent.parent
        # Go up to supercore/ root, then into squadOS/
        self.project_root = self.base_dir.parent.parent / "squadOS"

        # Documentation at squadOS/app-generation/documentation-base
        self.docs_dir = self.project_root / "app-generation" / "documentation-base"

        # Artifacts at squadOS/app-artefacts
        self.artifacts_dir = self.project_root / "app-artefacts"
        self.checkpoints_dir = self.base_dir / "state" / "checkpoints"

        # Input directories (all squads' outputs)
        self.frontend_dir = self.artifacts_dir / "engenharia" / "frontend"
        self.backend_dir = self.artifacts_dir / "engenharia" / "backend"
        self.product_dir = self.artifacts_dir / "produto"

        # Output directory
        self.qa_dir = self.artifacts_dir / "qa"
        self.reports_dir = self.qa_dir / "reports"
        self.bugs_dir = self.qa_dir / "bugs"
        self.approvals_dir = self.qa_dir / "approvals"

        for dir_path in [
            self.reports_dir,
            self.bugs_dir,
            self.approvals_dir,
            self.checkpoints_dir
        ]:
            dir_path.mkdir(parents=True, exist_ok=True)

        # Initialize delegator
        self.delegator = HybridDelegator(self.base_dir, self.project_root)

        # Cost tracking
        self.total_cost = 0.0

        logger.info("âœ… QA Owner Agent v2.0 (Skills-Only) initialized")
        logger.info(f"   Coverage threshold: {self.THRESHOLDS['coverage_minimum']}%")
        logger.info(f"   Quality threshold: {self.THRESHOLDS['quality_score_minimum']}/10")
        logger.info(f"   Artifacts: {self.qa_dir}")

    def execute_card(self, card_id: str, card_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Main execution method for QA validation

        Pattern (Skills-Only, 2-3 phases):
        1. Phase 1 (20-60%): verification-agent - Evidence validation
        2. Phase 2 (60-80%): llm-judge - Code quality scoring
        3. Phase 3 (80-90%): debugging-agent - Systematic debugging (if needed)

        Args:
            card_id: The card being validated (e.g., "PROD-001", "PROD-002", "PROD-003")
            card_data: Card metadata (title, acceptance_criteria, type, etc)

        Returns:
            Execution result with approval/rejection decision
        """
        logger.info(f"ðŸ§ª QA Owner Agent v2.0 validating card: {card_id}")
        start_time = datetime.now()

        # Check for existing checkpoint
        checkpoint = self._load_checkpoint(card_id)
        if checkpoint:
            logger.info(f"ðŸ“Œ Resuming from checkpoint: {checkpoint['stage']}")
            # Note: Resume functionality would be implemented in production

        try:
            # STAGE 1: ANALYZE CARD (15%)
            self._report_progress('card_analyzed', f"Analyzing {card_id}")
            card_type = self._determine_card_type(card_id, card_data)
            acceptance_criteria = card_data.get('acceptance_criteria', [])

            self._save_checkpoint(card_id, 'card_analyzed', {
                'card_type': card_type,
                'acceptance_criteria': acceptance_criteria
            })

            # STAGE 2: COLLECT ARTIFACTS (20%)
            self._report_progress('artifacts_collected', f"Collecting artifacts for {card_id}")
            artifacts = self._collect_artifacts(card_id, card_type)

            self._save_checkpoint(card_id, 'artifacts_collected', {
                'artifacts': {
                    'frontend': [str(p) for p in artifacts.get('frontend', [])],
                    'backend': [str(p) for p in artifacts.get('backend', [])],
                    'tests': [str(p) for p in artifacts.get('tests', [])]
                }
            })

            # PHASE 1: EVIDENCE VALIDATION (20-60%)
            self._report_progress('evidence_validated', f"Validating evidence via verification-agent")

            evidence_validation = self.delegator.validate_via_internal_skill(
                skill='verification-agent',
                task=f"Validate all tests passing for {card_id}",
                context={
                    'card_id': card_id,
                    'card_type': card_type,
                    'artifacts': artifacts,
                    'acceptance_criteria': acceptance_criteria,
                    'evidence': {
                        'test_results': self._gather_test_evidence(artifacts),
                        'lint_results': self._gather_lint_evidence(artifacts),
                        'build_results': self._gather_build_evidence(artifacts),
                        'coverage_results': self._gather_coverage_evidence(artifacts)
                    }
                }
            )

            self.total_cost += evidence_validation.get('cost_estimate', 0.05)

            self._save_checkpoint(card_id, 'evidence_validated', {
                'evidence_validation': {
                    'status': evidence_validation.get('status'),
                    'passed': evidence_validation.get('passed'),
                    'violations': evidence_validation.get('violations', [])
                }
            })

            # Check if evidence validation failed
            if not evidence_validation.get('passed', False):
                # Evidence validation failed - create correction card
                decision = self._make_decision(
                    evidence_validation={'passed': False, 'violations': evidence_validation.get('violations', [])},
                    quality_validation={'passed': False, 'score': 0},
                    debugging_result=None
                )

                report = self._generate_report(
                    card_id, card_data, evidence_validation,
                    {'passed': False, 'score': 0}, None, decision
                )

                return self._create_result(
                    card_id, decision, report, start_time
                )

            # PHASE 2: QUALITY EVALUATION (60-80%)
            self._report_progress('quality_evaluated', f"Evaluating code quality via llm-judge")

            quality_validation = self.delegator.validate_via_internal_skill(
                skill='llm-judge',
                task=f"Evaluate code quality for {card_id}",
                context={
                    'card_id': card_id,
                    'card_type': card_type,
                    'artifacts': artifacts,
                    'acceptance_criteria': acceptance_criteria,
                    'rubric_type': self._select_rubric(card_type)
                }
            )

            self.total_cost += quality_validation.get('cost_estimate', 0.10)

            self._save_checkpoint(card_id, 'quality_evaluated', {
                'quality_validation': {
                    'status': quality_validation.get('status'),
                    'passed': quality_validation.get('passed'),
                    'score': quality_validation.get('score', 0),
                    'feedback': quality_validation.get('feedback', '')
                }
            })

            # Check if quality score is below threshold
            quality_score = quality_validation.get('score', 0)
            quality_passed = quality_score >= self.THRESHOLDS['quality_score_minimum']

            debugging_result = None

            # PHASE 3: DEBUGGING (80-90%) - Only if quality failed
            if not quality_passed:
                self._report_progress('issues_debugged', f"Debugging issues via debugging-agent")

                debugging_result = self.delegator.validate_via_internal_skill(
                    skill='debugging-agent',
                    task=f"Debug quality issues for {card_id}",
                    context={
                        'card_id': card_id,
                        'card_type': card_type,
                        'artifacts': artifacts,
                        'quality_issues': quality_validation.get('issues', []),
                        'evidence': evidence_validation.get('evidence', {}),
                        'max_attempts': self.THRESHOLDS['max_debugging_attempts']
                    }
                )

                self.total_cost += debugging_result.get('cost_estimate', 0.15)

                self._save_checkpoint(card_id, 'issues_debugged', {
                    'debugging_result': {
                        'status': debugging_result.get('status'),
                        'root_cause': debugging_result.get('root_cause', ''),
                        'fix_applied': debugging_result.get('fix_applied', False)
                    }
                })

            # STAGE 6: MAKE DECISION (95%)
            self._report_progress('decision_made', f"Making approval/rejection decision")

            decision = self._make_decision(
                evidence_validation,
                quality_validation,
                debugging_result
            )

            # STAGE 7: COMPLETE (100%)
            self._report_progress('completed', f"QA validation complete for {card_id}")

            # Generate report
            report = self._generate_report(
                card_id,
                card_data,
                evidence_validation,
                quality_validation,
                debugging_result,
                decision
            )

            result = self._create_result(card_id, decision, report, start_time)

            # Clear checkpoint on completion
            self._clear_checkpoint(card_id)

            logger.info(f"âœ… QA validation complete: {card_id}")
            logger.info(f"   Decision: {decision['decision']}")
            logger.info(f"   Quality score: {quality_score:.1f}/10")
            logger.info(f"   Total cost: ${self.total_cost:.2f}")
            logger.info(f"   Time: {result['elapsed_time']:.2f}s")

            return result

        except Exception as e:
            logger.error(f"âŒ Error validating {card_id}: {e}")
            return {
                'status': 'failed',
                'card_id': card_id,
                'error': str(e),
                'elapsed_time': (datetime.now() - start_time).total_seconds()
            }

    def _determine_card_type(self, card_id: str, card_data: Dict[str, Any]) -> str:
        """
        Determine card type for appropriate validation

        Returns: 'Product', 'Architecture', 'Backend', 'Frontend', 'QA', 'Deploy'
        """
        # Check card metadata first
        card_type = card_data.get('type', '').lower()

        if 'backend' in card_type or 'api' in card_type:
            return 'Backend'
        elif 'frontend' in card_type or 'ui' in card_type or 'component' in card_type:
            return 'Frontend'
        elif 'architecture' in card_type or 'design' in card_type:
            return 'Architecture'
        elif 'product' in card_type or 'ux' in card_type:
            return 'Product'
        elif 'qa' in card_type or 'test' in card_type:
            return 'QA'
        elif 'deploy' in card_type or 'infra' in card_type:
            return 'Deploy'

        # Fallback: Detect from card pattern
        card_number = int(card_id.split('-')[1])

        # PROD-001, PROD-004, PROD-007... â†’ Product/Architecture
        if (card_number - 1) % 3 == 0:
            return 'Product'

        # PROD-002, PROD-005, PROD-008... â†’ Backend
        elif (card_number - 2) % 3 == 0:
            return 'Backend'

        # PROD-003, PROD-006, PROD-009... â†’ Frontend
        elif card_number % 3 == 0:
            return 'Frontend'

        return 'Backend'  # Default

    def _collect_artifacts(self, card_id: str, card_type: str) -> Dict[str, List[Path]]:
        """Collect generated artifacts based on card type"""
        artifacts = {
            'frontend': [],
            'backend': [],
            'tests': [],
            'docs': []
        }

        # Frontend artifacts
        if card_type in ['Frontend', 'Product']:
            if self.frontend_dir.exists():
                artifacts['frontend'] = (
                    list(self.frontend_dir.rglob("*.tsx")) +
                    list(self.frontend_dir.rglob("*.ts")) +
                    list(self.frontend_dir.rglob("*.jsx")) +
                    list(self.frontend_dir.rglob("*.js"))
                )

        # Backend artifacts
        if card_type in ['Backend', 'Architecture']:
            if self.backend_dir.exists():
                artifacts['backend'] = (
                    list(self.backend_dir.rglob("*.go")) +
                    list(self.backend_dir.rglob("*.py"))
                )

        # Test files (all types)
        test_patterns = ["*.test.tsx", "*.test.ts", "*_test.go", "test_*.py", "*.spec.ts"]
        for pattern in test_patterns:
            if self.frontend_dir.exists():
                artifacts['tests'].extend(self.frontend_dir.rglob(pattern))
            if self.backend_dir.exists():
                artifacts['tests'].extend(self.backend_dir.rglob(pattern))

        # Documentation
        if self.product_dir.exists():
            artifacts['docs'] = list(self.product_dir.rglob("*.md"))

        logger.info(f"   Collected artifacts:")
        logger.info(f"     Frontend: {len(artifacts['frontend'])} files")
        logger.info(f"     Backend: {len(artifacts['backend'])} files")
        logger.info(f"     Tests: {len(artifacts['tests'])} files")
        logger.info(f"     Docs: {len(artifacts['docs'])} files")

        return artifacts

    def _gather_test_evidence(self, artifacts: Dict[str, List[Path]]) -> str:
        """Gather test execution evidence (would run actual tests in production)"""
        # In production, this would execute:
        # - npm test (Frontend)
        # - pytest (Python Backend)
        # - go test ./... (Go Backend)
        # - npx playwright test (E2E)

        test_files = artifacts.get('tests', [])
        return f"Found {len(test_files)} test files (simulated: all passing)"

    def _gather_lint_evidence(self, artifacts: Dict[str, List[Path]]) -> str:
        """Gather lint evidence (would run actual linters in production)"""
        # In production, this would execute:
        # - npm run lint (Frontend)
        # - pylint/flake8 (Python Backend)
        # - golangci-lint (Go Backend)

        return "Linting: No issues found (simulated)"

    def _gather_build_evidence(self, artifacts: Dict[str, List[Path]]) -> str:
        """Gather build evidence (would run actual builds in production)"""
        # In production, this would execute:
        # - npm run build (Frontend)
        # - go build (Go Backend)
        # - python -m compileall (Python Backend)

        return "Build: Success (simulated)"

    def _gather_coverage_evidence(self, artifacts: Dict[str, List[Path]]) -> str:
        """Gather coverage evidence (would run actual coverage tools in production)"""
        # In production, this would execute:
        # - npm run coverage (Frontend - Jest)
        # - pytest --cov (Python Backend)
        # - go test -cover (Go Backend)

        return "Coverage: 85% (simulated)"

    def _select_rubric(self, card_type: str) -> str:
        """Select appropriate LLM-as-Judge rubric based on card type"""
        rubric_mapping = {
            'Backend': 'backend_code_quality',
            'Frontend': 'frontend_code_quality',
            'Architecture': 'architecture_compliance',
            'Product': 'frontend_code_quality',  # Product often has UX
            'QA': 'backend_code_quality',        # Default
            'Deploy': 'backend_code_quality'     # Default
        }

        return rubric_mapping.get(card_type, 'backend_code_quality')

    def _make_decision(
        self,
        evidence_validation: Dict[str, Any],
        quality_validation: Dict[str, Any],
        debugging_result: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Make approval/rejection decision

        Approval criteria:
        1. Evidence validation passed (all tests passing, lint clean, build success, coverage â‰¥80%)
        2. Quality score â‰¥ 8.0/10 (LLM-as-Judge)
        3. If debugging was attempted, fix must be applied successfully

        Returns:
            {
                'status': 'approved' | 'rejected',
                'decision': 'APPROVED' | 'REJECTED',
                'reasons': [str],
                'next_action': 'proceed_to_deploy' | 'create_correction_card'
            }
        """
        evidence_passed = evidence_validation.get('passed', False)
        quality_score = quality_validation.get('score', 0)
        quality_passed = quality_score >= self.THRESHOLDS['quality_score_minimum']

        # Check if debugging was attempted and successful
        debugging_fixed = True
        if debugging_result:
            debugging_fixed = debugging_result.get('fix_applied', False)

        should_approve = evidence_passed and quality_passed and debugging_fixed

        if should_approve:
            decision = {
                'status': 'approved',
                'decision': 'APPROVED',
                'reasons': [
                    'All tests passed',
                    f"Quality score: {quality_score:.1f}/10 (â‰¥{self.THRESHOLDS['quality_score_minimum']})",
                    'Zero-tolerance compliant',
                    'All acceptance criteria met'
                ],
                'next_action': 'proceed_to_deploy'
            }
        else:
            reasons = []

            if not evidence_passed:
                violations = evidence_validation.get('violations', [])
                for violation in violations[:3]:  # Show first 3 violations
                    reasons.append(f"Evidence: {violation}")

            if not quality_passed:
                reasons.append(f"Quality score: {quality_score:.1f}/10 (<{self.THRESHOLDS['quality_score_minimum']})")
                feedback = quality_validation.get('feedback', '')
                if feedback:
                    reasons.append(f"Feedback: {feedback[:100]}")

            if debugging_result and not debugging_fixed:
                reasons.append("Debugging failed to resolve issues")

            decision = {
                'status': 'rejected',
                'decision': 'REJECTED',
                'reasons': reasons,
                'next_action': 'create_correction_card'
            }

        return decision

    def _generate_report(
        self,
        card_id: str,
        card_data: Dict[str, Any],
        evidence_validation: Dict[str, Any],
        quality_validation: Dict[str, Any],
        debugging_result: Optional[Dict[str, Any]],
        decision: Dict[str, Any]
    ) -> Path:
        """Generate comprehensive QA report"""
        report_file = self.reports_dir / f"{card_id}_qa_report_v2.md"

        report_content = f'''# QA Report v2.0 - {card_id}

**Date**: {datetime.now().isoformat()}
**Card**: {card_data.get('title', 'N/A')}
**Decision**: {decision['decision']}
**Quality Score**: {quality_validation.get('score', 0):.1f}/10

---

## Summary

- **Evidence Validation**: {'âœ… PASSED' if evidence_validation.get('passed') else 'âŒ FAILED'}
- **Quality Score**: {quality_validation.get('score', 0):.1f}/10 (threshold: {self.THRESHOLDS['quality_score_minimum']})
- **Debugging Attempted**: {'Yes' if debugging_result else 'No'}
- **Total Cost**: ${self.total_cost:.2f}

---

## Phase 1: Evidence Validation (verification-agent)

**Status**: {'âœ… PASSED' if evidence_validation.get('passed') else 'âŒ FAILED'}

**Evidence**:
{evidence_validation.get('evidence', 'No evidence available')}

**Violations** ({len(evidence_validation.get('violations', []))}):
'''

        for violation in evidence_validation.get('violations', []):
            report_content += f"- {violation}\n"

        report_content += f'''
---

## Phase 2: Quality Evaluation (llm-judge)

**Status**: {'âœ… PASSED' if quality_validation.get('passed') else 'âŒ FAILED'}
**Score**: {quality_validation.get('score', 0):.1f}/10

**Feedback**:
{quality_validation.get('feedback', 'No feedback available')}

**Dimensions**:
'''

        dimensions = quality_validation.get('dimensions', {})
        for dimension, score in dimensions.items():
            report_content += f"- {dimension}: {score:.1f}/10\n"

        if debugging_result:
            report_content += f'''
---

## Phase 3: Debugging (debugging-agent)

**Status**: {'âœ… FIXED' if debugging_result.get('fix_applied') else 'âŒ NOT FIXED'}

**Root Cause**:
{debugging_result.get('root_cause', 'No root cause identified')}

**Fix Applied**: {debugging_result.get('fix_applied', False)}

**Attempts**: {debugging_result.get('attempts', 0)}/{self.THRESHOLDS['max_debugging_attempts']}
'''

        report_content += f'''
---

## Decision

**{decision['decision']}**

### Reasons

'''

        for reason in decision['reasons']:
            report_content += f"- {reason}\n"

        report_content += f'''
---

## Next Action

{decision['next_action']}

---

**Generated by**: QA Owner Agent v2.0 (Skills-Only)
**Date**: {datetime.now().isoformat()}
**Total Cost**: ${self.total_cost:.2f}
'''

        report_file.write_text(report_content, encoding='utf-8')
        logger.info(f"   Report generated: {report_file.name}")

        return report_file

    def _create_result(
        self,
        card_id: str,
        decision: Dict[str, Any],
        report: Path,
        start_time: datetime
    ) -> Dict[str, Any]:
        """Create standardized result dictionary"""
        elapsed_time = (datetime.now() - start_time).total_seconds()

        return {
            'status': decision['status'],
            'card_id': card_id,
            'decision': decision['decision'],
            'reasons': decision['reasons'],
            'next_action': decision['next_action'],
            'report_path': str(report),
            'total_cost': round(self.total_cost, 2),
            'elapsed_time': elapsed_time
        }

    # Checkpoint methods (same pattern as other agents)

    def _save_checkpoint(self, card_id: str, stage: str, data: Dict[str, Any]):
        """Save checkpoint for resumability"""
        checkpoint_file = self.checkpoints_dir / f"{card_id}_qa_v2.json"
        checkpoint = {
            'card_id': card_id,
            'stage': stage,
            'timestamp': datetime.now().isoformat(),
            'data': data
        }
        checkpoint_file.write_text(json.dumps(checkpoint, indent=2), encoding='utf-8')
        logger.debug(f"ðŸ’¾ Checkpoint saved: {stage}")

    def _load_checkpoint(self, card_id: str) -> Optional[Dict[str, Any]]:
        """Load checkpoint if exists"""
        checkpoint_file = self.checkpoints_dir / f"{card_id}_qa_v2.json"
        if checkpoint_file.exists():
            return json.loads(checkpoint_file.read_text(encoding='utf-8'))
        return None

    def _clear_checkpoint(self, card_id: str):
        """Clear checkpoint after successful completion"""
        checkpoint_file = self.checkpoints_dir / f"{card_id}_qa_v2.json"
        if checkpoint_file.exists():
            checkpoint_file.unlink()
            logger.debug(f"ðŸ—‘ï¸  Checkpoint cleared")

    def _report_progress(self, stage: str, message: str):
        """Report progress (would send to monitoring system in production)"""
        progress = self.STAGES.get(stage, 0)
        logger.info(f"ðŸ“Š Progress: {progress}% - {message}")


def create_qa_owner_agent() -> QAOwnerAgentV2:
    """Factory function to create QA Owner Agent v2.0"""
    return QAOwnerAgentV2()


def main():
    """Test execution (for development)"""
    agent = create_qa_owner_agent()

    # Test card
    test_card = {
        'card_id': 'PROD-003',
        'title': 'Implement Oracle Management UI',
        'type': 'Frontend',
        'requirement_ids': ['RF001'],
        'acceptance_criteria': [
            'UI displays list of oracles',
            'User can create new oracle',
            'Tests pass with â‰¥80% coverage',
            'Zero vulnerabilities',
            'Quality score â‰¥8.0'
        ]
    }

    result = agent.execute_card('PROD-003', test_card)
    print(json.dumps(result, indent=2, default=str))


if __name__ == '__main__':
    main()
