#!/usr/bin/env python3
"""
Infrastructure Owner Agent v1.0.0

PHASE 0: Sets up infrastructure before any code generation.

Analyzes stack_supercore_v2.0.md and generates:
- docker-compose.yml (PostgreSQL, Redis, NebulaGraph, MinIO)
- Dockerfiles (backend Go/Python, frontend Node.js)
- Environment configuration (.env.example)
- Database initialization scripts
- Service startup scripts
- Infrastructure documentation

Based on Architecture Owner Agent v1.0 (agent-first architecture).

Author: SquadOS Meta-Framework
Date: 2025-12-27
"""

import json
import logging
import re
import yaml
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class InfrastructureOwnerAgent:
    """
    Infrastructure Owner Agent - Sets up infrastructure for the generated solution.

    Features:
    - Agent-first architecture (direct parsing, no LLM calls)
    - Checkpoint system for resumability
    - Progress reporting (6 stages: 20%, 40%, 60%, 75%, 90%, 100%)
    - Comprehensive validation
    - Infrastructure generation (Docker, databases, scripts)
    """

    # Progress stages
    STAGES = {
        'stack_parsed': 20,
        'docker_compose_generated': 40,
        'dockerfiles_generated': 60,
        'scripts_generated': 75,
        'validated': 90,
        'completed': 100
    }

    def __init__(self):
        """Initialize Infrastructure Owner Agent"""
        # Paths
        self.base_dir = Path(__file__).parent.parent
        # Documentation is at project root/documentation-base (not app-generation/)
        self.docs_dir = self.base_dir.parent.parent / "documentation-base"
        self.artifacts_dir = self.base_dir.parent / "app-artefacts" / "infrastructure"
        self.solution_dir = self.base_dir.parent.parent / "app-solution"
        self.checkpoints_dir = self.base_dir / "state" / "checkpoints"

        # Documentation files
        self.stack_doc = self.docs_dir / "stack_supercore_v2.0.md"

        # Create output directories
        self.docker_dir = self.artifacts_dir / "docker"
        self.scripts_dir = self.artifacts_dir / "scripts"
        self.docs_infra_dir = self.artifacts_dir / "docs"

        for dir_path in [
            self.artifacts_dir,
            self.docker_dir,
            self.scripts_dir,
            self.docs_infra_dir,
            self.checkpoints_dir,
            self.solution_dir
        ]:
            dir_path.mkdir(parents=True, exist_ok=True)

        logger.info("âœ… Infrastructure Owner Agent initialized")
        logger.info(f"   Documentation: {self.docs_dir}")
        logger.info(f"   Artifacts: {self.artifacts_dir}")
        logger.info(f"   Solution: {self.solution_dir}")

    def execute(self) -> Dict[str, Any]:
        """
        Main execution method for PHASE 0 infrastructure setup

        Returns:
            Execution result with generated infrastructure files
        """
        logger.info(f"ğŸ—ï¸ Infrastructure Owner Agent executing PHASE 0")
        start_time = datetime.now()

        # Check for existing checkpoint
        checkpoint = self._load_checkpoint('INFRA-001')
        if checkpoint:
            logger.info(f"ğŸ“Œ Resuming from checkpoint: {checkpoint['stage']}")
            return self._resume_from_checkpoint('INFRA-001', checkpoint)

        try:
            # Stage 1: Parse stack documentation (20%)
            self._report_progress('stack_parsed', "Parsing stack technologies")
            stack_data = self._parse_stack_doc()

            self._save_checkpoint('INFRA-001', 'stack_parsed', {
                'stack_data': stack_data
            })

            # Stage 2: Generate docker-compose.yml (40%)
            self._report_progress('docker_compose_generated', "Generating docker-compose.yml")
            compose_file = self._generate_docker_compose(stack_data)

            self._save_checkpoint('INFRA-001', 'docker_compose_generated', {
                'compose_file': compose_file
            })

            # Stage 3: Generate Dockerfiles (60%)
            self._report_progress('dockerfiles_generated', "Generating Dockerfiles")
            dockerfiles = self._generate_dockerfiles(stack_data)

            self._save_checkpoint('INFRA-001', 'dockerfiles_generated', {
                'dockerfiles': dockerfiles
            })

            # Stage 4: Generate scripts (75%)
            self._report_progress('scripts_generated', "Generating setup scripts")
            scripts = self._generate_scripts(stack_data)

            self._save_checkpoint('INFRA-001', 'scripts_generated', {
                'scripts': scripts
            })

            # Stage 5: Validate (90%)
            self._report_progress('validated', "Validating infrastructure files")
            validation_result = self._validate_infrastructure(
                compose_file, dockerfiles, scripts
            )

            if not validation_result:
                raise ValueError("Infrastructure validation failed")

            # Stage 6: Complete (100%)
            self._report_progress('completed', "Infrastructure setup complete")

            duration = (datetime.now() - start_time).total_seconds()

            all_artifacts = [compose_file] + dockerfiles + scripts

            result = {
                'status': 'completed',
                'phase': 'PHASE 0',
                'artifacts': all_artifacts,
                'duration_seconds': duration,
                'timestamp': datetime.now().isoformat()
            }

            # Clean up checkpoint
            self._delete_checkpoint('INFRA-001')

            logger.info(f"âœ… Infrastructure setup completed in {duration:.2f}s")
            logger.info(f"   Generated {len(all_artifacts)} infrastructure files")

            return result

        except Exception as e:
            logger.error(f"âŒ Infrastructure setup failed: {e}", exc_info=True)
            return {
                'status': 'failed',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }

    def _parse_stack_doc(self) -> Dict[str, Any]:
        """
        Parse stack_supercore_v2.0.md to extract technologies

        Returns:
            Dictionary with technologies categorized by type
        """
        logger.info("ğŸ“„ Parsing stack documentation")

        if not self.stack_doc.exists():
            raise FileNotFoundError(f"Stack documentation not found: {self.stack_doc}")

        content = self.stack_doc.read_text(encoding='utf-8')

        stack_data = {
            'databases': [],
            'backend': [],
            'frontend': [],
            'infrastructure': [],
            'messaging': [],
            'storage': [],
            'monitoring': [],
            'ai_ml': []
        }

        # Parse databases
        if 'PostgreSQL' in content:
            stack_data['databases'].append({
                'name': 'PostgreSQL',
                'version': '16',
                'port': 5432,
                'description': 'Relational database'
            })

        if 'Redis' in content:
            stack_data['messaging'].append({
                'name': 'Redis',
                'version': '7',
                'port': 6379,
                'description': 'Cache and message broker'
            })

        if 'NebulaGraph' in content or 'Nebula Graph' in content:
            stack_data['databases'].append({
                'name': 'NebulaGraph',
                'version': '3.6',
                'ports': {
                    'graphd': 9669,
                    'metad': 9559,
                    'storaged': 9779
                },
                'description': 'Graph database'
            })

        if 'MinIO' in content or 'Minio' in content:
            stack_data['storage'].append({
                'name': 'MinIO',
                'version': 'latest',
                'port': 9000,
                'console_port': 9001,
                'description': 'S3-compatible object storage'
            })

        # Parse backend technologies
        if 'Go' in content or 'Golang' in content:
            stack_data['backend'].append({
                'name': 'Go',
                'version': '1.21',
                'description': 'Backend API language'
            })

        if 'Python' in content and 'FastAPI' in content:
            stack_data['backend'].append({
                'name': 'Python',
                'version': '3.11',
                'framework': 'FastAPI',
                'description': 'Backend API framework'
            })

        # Parse frontend technologies
        if 'Next.js' in content or 'NextJS' in content:
            stack_data['frontend'].append({
                'name': 'Next.js',
                'version': '14',
                'description': 'React framework'
            })

        if 'Node.js' in content or 'NodeJS' in content:
            stack_data['frontend'].append({
                'name': 'Node.js',
                'version': '20',
                'description': 'JavaScript runtime'
            })

        # Parse AI/ML technologies
        if 'Qdrant' in content:
            stack_data['ai_ml'].append({
                'name': 'Qdrant',
                'version': 'latest',
                'port': 6333,
                'description': 'Vector database'
            })

        logger.info(f"   Databases: {len(stack_data['databases'])}")
        logger.info(f"   Backend: {len(stack_data['backend'])}")
        logger.info(f"   Frontend: {len(stack_data['frontend'])}")
        logger.info(f"   AI/ML: {len(stack_data['ai_ml'])}")

        return stack_data

    def _generate_docker_compose(self, stack_data: Dict[str, Any]) -> Dict[str, str]:
        """Generate docker-compose.yml"""
        logger.info("ğŸ³ Generating docker-compose.yml")

        services = {}
        networks = {'supercore-network': {'driver': 'bridge'}}
        volumes = {}

        # PostgreSQL
        for db in stack_data.get('databases', []):
            if db['name'] == 'PostgreSQL':
                services['postgres'] = {
                    'image': f"postgres:{db['version']}",
                    'container_name': 'supercore-postgres',
                    'environment': {
                        'POSTGRES_DB': 'supercore',
                        'POSTGRES_USER': 'supercore_user',
                        'POSTGRES_PASSWORD': 'supercore_password',
                        'PGDATA': '/var/lib/postgresql/data/pgdata'
                    },
                    'ports': [f"{db['port']}:{db['port']}"],
                    'volumes': ['postgres-data:/var/lib/postgresql/data'],
                    'networks': ['supercore-network'],
                    'healthcheck': {
                        'test': ['CMD-SHELL', 'pg_isready -U supercore_user'],
                        'interval': '10s',
                        'timeout': '5s',
                        'retries': 5
                    }
                }
                volumes['postgres-data'] = {}

            elif db['name'] == 'NebulaGraph':
                # Nebula Graph requires 3 services
                services['nebula-metad'] = {
                    'image': f"vesoft/nebula-metad:v{db['version']}",
                    'container_name': 'supercore-nebula-metad',
                    'ports': [f"{db['ports']['metad']}:{db['ports']['metad']}"],
                    'volumes': ['nebula-meta:/data/meta'],
                    'networks': ['supercore-network'],
                    'command': '--meta_server_addrs=nebula-metad:9559 --local_ip=nebula-metad --ws_ip=nebula-metad --port=9559'
                }

                services['nebula-storaged'] = {
                    'image': f"vesoft/nebula-storaged:v{db['version']}",
                    'container_name': 'supercore-nebula-storaged',
                    'ports': [f"{db['ports']['storaged']}:{db['ports']['storaged']}"],
                    'volumes': ['nebula-storage:/data/storage'],
                    'networks': ['supercore-network'],
                    'depends_on': ['nebula-metad'],
                    'command': '--meta_server_addrs=nebula-metad:9559 --local_ip=nebula-storaged --ws_ip=nebula-storaged --port=9779'
                }

                services['nebula-graphd'] = {
                    'image': f"vesoft/nebula-graphd:v{db['version']}",
                    'container_name': 'supercore-nebula-graphd',
                    'ports': [f"{db['ports']['graphd']}:{db['ports']['graphd']}"],
                    'networks': ['supercore-network'],
                    'depends_on': ['nebula-metad', 'nebula-storaged'],
                    'command': '--meta_server_addrs=nebula-metad:9559 --local_ip=nebula-graphd --ws_ip=nebula-graphd --port=9669'
                }

                volumes['nebula-meta'] = {}
                volumes['nebula-storage'] = {}

        # Redis
        for msg in stack_data.get('messaging', []):
            if msg['name'] == 'Redis':
                services['redis'] = {
                    'image': f"redis:{msg['version']}-alpine",
                    'container_name': 'supercore-redis',
                    'ports': [f"{msg['port']}:{msg['port']}"],
                    'volumes': ['redis-data:/data'],
                    'networks': ['supercore-network'],
                    'command': 'redis-server --appendonly yes',
                    'healthcheck': {
                        'test': ['CMD', 'redis-cli', 'ping'],
                        'interval': '10s',
                        'timeout': '5s',
                        'retries': 5
                    }
                }
                volumes['redis-data'] = {}

        # MinIO
        for storage in stack_data.get('storage', []):
            if storage['name'] == 'MinIO':
                services['minio'] = {
                    'image': 'minio/minio:latest',
                    'container_name': 'supercore-minio',
                    'ports': [
                        f"{storage['port']}:{storage['port']}",
                        f"{storage['console_port']}:{storage['console_port']}"
                    ],
                    'volumes': ['minio-data:/data'],
                    'networks': ['supercore-network'],
                    'environment': {
                        'MINIO_ROOT_USER': 'minioadmin',
                        'MINIO_ROOT_PASSWORD': 'minioadmin123'
                    },
                    'command': 'server /data --console-address ":9001"',
                    'healthcheck': {
                        'test': ['CMD', 'curl', '-f', 'http://localhost:9000/minio/health/live'],
                        'interval': '30s',
                        'timeout': '10s',
                        'retries': 3
                    }
                }
                volumes['minio-data'] = {}

        # Qdrant
        for ai in stack_data.get('ai_ml', []):
            if ai['name'] == 'Qdrant':
                services['qdrant'] = {
                    'image': 'qdrant/qdrant:latest',
                    'container_name': 'supercore-qdrant',
                    'ports': [f"{ai['port']}:{ai['port']}"],
                    'volumes': ['qdrant-data:/qdrant/storage'],
                    'networks': ['supercore-network']
                }
                volumes['qdrant-data'] = {}

        compose_config = {
            'version': '3.8',
            'services': services,
            'networks': networks,
            'volumes': volumes
        }

        # Write docker-compose.yml to solution root
        compose_path = self.solution_dir / "docker-compose.yml"
        with open(compose_path, 'w', encoding='utf-8') as f:
            yaml.dump(compose_config, f, default_flow_style=False, sort_keys=False)

        logger.info(f"   âœ… docker-compose.yml: {len(services)} services")

        return {
            'type': 'docker-compose',
            'path': str(compose_path),
            'services_count': len(services)
        }

    def _generate_dockerfiles(self, stack_data: Dict[str, Any]) -> List[Dict[str, str]]:
        """Generate Dockerfiles for backend and frontend"""
        logger.info("ğŸ³ Generating Dockerfiles")

        dockerfiles = []

        # Backend Dockerfile (Go + Python)
        backend_dockerfile = """# Backend Dockerfile - Go + Python (FastAPI)
FROM golang:1.21-alpine AS go-builder

WORKDIR /app
COPY backend/go.mod backend/go.sum ./
RUN go mod download

COPY backend/ .
RUN CGO_ENABLED=0 GOOS=linux go build -o /supercore-api ./cmd/api

# Python FastAPI stage
FROM python:3.11-slim

WORKDIR /app

# Install Go binary
COPY --from=go-builder /supercore-api /usr/local/bin/

# Install Python dependencies
COPY backend/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY backend/ .

# Expose ports
EXPOSE 8000 8080

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\
  CMD curl -f http://localhost:8000/health || exit 1

CMD ["python", "main.py"]
"""

        backend_path = self.solution_dir / "Dockerfile.backend"
        backend_path.write_text(backend_dockerfile, encoding='utf-8')

        dockerfiles.append({
            'type': 'dockerfile',
            'service': 'backend',
            'path': str(backend_path)
        })

        # Frontend Dockerfile (Next.js)
        frontend_dockerfile = """# Frontend Dockerfile - Next.js
FROM node:20-alpine AS builder

WORKDIR /app

# Install dependencies
COPY frontend/package*.json ./
RUN npm ci

# Copy source
COPY frontend/ .

# Build
RUN npm run build

# Production image
FROM node:20-alpine AS runner

WORKDIR /app

ENV NODE_ENV production

# Copy built app
COPY --from=builder /app/.next ./.next
COPY --from=builder /app/public ./public
COPY --from=builder /app/package*.json ./
COPY --from=builder /app/node_modules ./node_modules

# Expose port
EXPOSE 3000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\
  CMD wget --no-verbose --tries=1 --spider http://localhost:3000 || exit 1

CMD ["npm", "start"]
"""

        frontend_path = self.solution_dir / "Dockerfile.frontend"
        frontend_path.write_text(frontend_dockerfile, encoding='utf-8')

        dockerfiles.append({
            'type': 'dockerfile',
            'service': 'frontend',
            'path': str(frontend_path)
        })

        logger.info(f"   âœ… Generated {len(dockerfiles)} Dockerfiles")

        return dockerfiles

    def _generate_scripts(self, stack_data: Dict[str, Any]) -> List[Dict[str, str]]:
        """Generate setup and utility scripts"""
        logger.info("ğŸ“œ Generating setup scripts")

        scripts = []

        # 1. Setup database script
        setup_db_script = """#!/bin/bash
# Setup PostgreSQL database for SuperCore

set -e

echo "ğŸ—„ï¸  Setting up PostgreSQL database..."

# Wait for PostgreSQL to be ready
until docker exec supercore-postgres pg_isready -U supercore_user; do
  echo "Waiting for PostgreSQL..."
  sleep 2
done

echo "âœ… PostgreSQL is ready"

# Create extensions
docker exec supercore-postgres psql -U supercore_user -d supercore -c "CREATE EXTENSION IF NOT EXISTS pg_trgm;"
docker exec supercore-postgres psql -U supercore_user -d supercore -c "CREATE EXTENSION IF NOT EXISTS btree_gin;"
docker exec supercore-postgres psql -U supercore_user -d supercore -c "CREATE EXTENSION IF NOT EXISTS uuid-ossp;"

echo "âœ… PostgreSQL extensions created"

# Run migrations (if they exist)
if [ -d "./backend/migrations" ]; then
  echo "ğŸ”„ Running database migrations..."
  # Add migration command here
  echo "âœ… Migrations completed"
fi

echo "ğŸ‰ Database setup complete!"
"""

        setup_db_path = self.solution_dir / "scripts" / "setup-db.sh"
        setup_db_path.parent.mkdir(parents=True, exist_ok=True)
        setup_db_path.write_text(setup_db_script, encoding='utf-8')
        setup_db_path.chmod(0o755)

        scripts.append({
            'type': 'script',
            'name': 'setup-db.sh',
            'path': str(setup_db_path)
        })

        # 2. Start services script
        start_services_script = """#!/bin/bash
# Start all SuperCore services

set -e

echo "ğŸš€ Starting SuperCore services..."

# Start infrastructure
echo "ğŸ“¦ Starting infrastructure (PostgreSQL, Redis, NebulaGraph, MinIO, Qdrant)..."
docker-compose up -d

# Wait for services
echo "â³ Waiting for services to be healthy..."
sleep 10

# Setup database
echo "ğŸ—„ï¸  Setting up database..."
./scripts/setup-db.sh

# Setup NebulaGraph (if needed)
if docker ps | grep -q nebula-graphd; then
  echo "ğŸ“Š Setting up NebulaGraph..."
  # Add NebulaGraph setup commands here
  echo "âœ… NebulaGraph ready"
fi

# Setup MinIO buckets (if needed)
if docker ps | grep -q supercore-minio; then
  echo "ğŸ“¦ Setting up MinIO buckets..."
  # Add MinIO bucket creation here
  echo "âœ… MinIO buckets ready"
fi

echo "âœ… All services started successfully!"
echo ""
echo "ğŸ“Š Service URLs:"
echo "   PostgreSQL: localhost:5432"
echo "   Redis: localhost:6379"
echo "   NebulaGraph: localhost:9669"
echo "   MinIO API: localhost:9000"
echo "   MinIO Console: localhost:9001"
echo "   Qdrant: localhost:6333"
echo ""
echo "ğŸ‰ SuperCore is ready!"
"""

        start_services_path = self.solution_dir / "scripts" / "start-services.sh"
        start_services_path.write_text(start_services_script, encoding='utf-8')
        start_services_path.chmod(0o755)

        scripts.append({
            'type': 'script',
            'name': 'start-services.sh',
            'path': str(start_services_path)
        })

        # 3. Stop services script
        stop_services_script = """#!/bin/bash
# Stop all SuperCore services

set -e

echo "ğŸ›‘ Stopping SuperCore services..."

docker-compose down

echo "âœ… All services stopped"
"""

        stop_services_path = self.solution_dir / "scripts" / "stop-services.sh"
        stop_services_path.write_text(stop_services_script, encoding='utf-8')
        stop_services_path.chmod(0o755)

        scripts.append({
            'type': 'script',
            'name': 'stop-services.sh',
            'path': str(stop_services_path)
        })

        # 4. Environment template
        env_template = """# SuperCore Environment Configuration

# Database
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=supercore
POSTGRES_USER=supercore_user
POSTGRES_PASSWORD=supercore_password

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379

# NebulaGraph
NEBULA_GRAPHD_HOST=localhost
NEBULA_GRAPHD_PORT=9669
NEBULA_SPACE=supercore

# MinIO
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin123
MINIO_BUCKET=supercore

# Qdrant
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Application
APP_ENV=development
API_PORT=8000
FRONTEND_PORT=3000

# Logging
LOG_LEVEL=INFO
"""

        env_path = self.solution_dir / ".env.example"
        env_path.write_text(env_template, encoding='utf-8')

        scripts.append({
            'type': 'env-template',
            'name': '.env.example',
            'path': str(env_path)
        })

        logger.info(f"   âœ… Generated {len(scripts)} scripts and templates")

        return scripts

    def _validate_infrastructure(
        self,
        compose_file: Dict[str, str],
        dockerfiles: List[Dict[str, str]],
        scripts: List[Dict[str, str]]
    ) -> bool:
        """Validate all infrastructure files"""
        logger.info("ğŸ” Validating infrastructure files")

        # Check docker-compose.yml exists
        compose_path = Path(compose_file['path'])
        if not compose_path.exists():
            logger.error(f"   âŒ docker-compose.yml not found: {compose_path}")
            return False

        # Check Dockerfiles exist
        for dockerfile in dockerfiles:
            df_path = Path(dockerfile['path'])
            if not df_path.exists():
                logger.error(f"   âŒ Dockerfile not found: {df_path}")
                return False

        # Check scripts exist and are executable
        for script in scripts:
            script_path = Path(script['path'])
            if not script_path.exists():
                logger.error(f"   âŒ Script not found: {script_path}")
                return False

            # Skip executable check for .env.example
            if script['type'] == 'script' and not script_path.stat().st_mode & 0o111:
                logger.error(f"   âŒ Script not executable: {script_path}")
                return False

        logger.info("   âœ… All infrastructure files validated")
        return True

    def _report_progress(self, stage: str, message: str):
        """Report progress to meta-orchestrator"""
        progress = self.STAGES.get(stage, 0)
        logger.info(f"   [{progress}%] {message}")

    def _save_checkpoint(self, checkpoint_id: str, stage: str, data: Dict[str, Any]):
        """Save checkpoint for resumability"""
        checkpoint_file = self.checkpoints_dir / f"{checkpoint_id}.json"
        checkpoint_data = {
            'stage': stage,
            'progress': self.STAGES.get(stage, 0),
            'data': data,
            'timestamp': datetime.now().isoformat()
        }
        checkpoint_file.write_text(json.dumps(checkpoint_data, indent=2), encoding='utf-8')

    def _load_checkpoint(self, checkpoint_id: str) -> Optional[Dict[str, Any]]:
        """Load checkpoint if exists"""
        checkpoint_file = self.checkpoints_dir / f"{checkpoint_id}.json"
        if checkpoint_file.exists():
            return json.loads(checkpoint_file.read_text(encoding='utf-8'))
        return None

    def _delete_checkpoint(self, checkpoint_id: str):
        """Delete checkpoint after completion"""
        checkpoint_file = self.checkpoints_dir / f"{checkpoint_id}.json"
        if checkpoint_file.exists():
            checkpoint_file.unlink()

    def _resume_from_checkpoint(self, checkpoint_id: str, checkpoint: Dict[str, Any]) -> Dict[str, Any]:
        """Resume execution from checkpoint"""
        logger.info(f"ğŸ“Œ Resuming from stage: {checkpoint['stage']}")

        # For now, just return the checkpoint data
        # In production, you would resume from the specific stage
        return {
            'status': 'resumed',
            'checkpoint': checkpoint,
            'timestamp': datetime.now().isoformat()
        }


if __name__ == '__main__':
    """
    Test Infrastructure Owner Agent standalone

    Usage:
        python3 infrastructure_owner_agent.py
    """
    try:
        agent = InfrastructureOwnerAgent()
        result = agent.execute()

        print("\n" + "="*80)
        print("ğŸ‰ Infrastructure Owner Agent Test Complete")
        print("="*80)
        print(f"Status: {result['status']}")
        print(f"Artifacts: {len(result.get('artifacts', []))}")
        print(f"Duration: {result.get('duration_seconds', 0):.2f}s")

        if result['status'] == 'completed':
            print("\nğŸ“¦ Generated Files:")
            for artifact in result.get('artifacts', []):
                print(f"   âœ… {artifact['type']}: {Path(artifact['path']).name}")

    except Exception as e:
        logger.error(f"âŒ Test failed: {e}", exc_info=True)
        exit(1)
