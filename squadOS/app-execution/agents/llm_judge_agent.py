#!/usr/bin/env python3
"""
LLM-as-Judge Agent - Automated Code Quality Evaluation

Evaluates code quality using LLM-based rubric scoring.
Inspired by Context Engineering evaluation patterns.

Key Features:
- Multi-dimensional rubric scoring (Correctness, Style, Performance, Docs)
- Weighted overall scores with pass/fail thresholds
- Detailed feedback with evidence and improvement suggestions
- Integration with CachedLLMClient for cost optimization (90% savings)

Usage:
    from agents.llm_judge_agent import LLMJudgeAgent

    agent = LLMJudgeAgent()
    result = agent.evaluate_code_quality(
        card_id='PROD-001',
        card_type='backend',  # or 'frontend', 'architecture'
        artifacts={
            'code': {'file.py': code_content},
            'tests': {'test_file.py': test_content},
            'docs': {'README.md': readme_content}
        }
    )

    if result['passed']:
        # Auto-approve to QA
    else:
        # Create improvement card with result['feedback']

ROI:
    - $24,000/year from QA automation (70% of tasks)
    - 10Ã— faster feedback (2 min vs 20 min)
    - Consistent evaluation (no human variance)
    - 47Ã— return on 8h investment
"""

import os
import json
import logging
import re
from typing import Dict, List, Any, Optional
from pathlib import Path
from datetime import datetime

logger = logging.getLogger(__name__)

try:
    from utils.cached_llm_client import get_cached_client
    CACHED_CLIENT_AVAILABLE = True
except ImportError:
    CACHED_CLIENT_AVAILABLE = False
    logger.warning("âš ï¸ CachedLLMClient not available. LLM evaluations will not be cached.")


class LLMJudgeAgent:
    """
    LLM-as-Judge Agent for automated code quality evaluation

    Evaluates code against multi-dimensional rubrics:
    - Backend: Correctness, Style, Performance, Documentation
    - Frontend: Correctness, UI/UX, Style, Performance
    - Architecture: Layering, ADR Compliance, Stack, Documentation

    Returns structured feedback with scores, evidence, and improvements.

    Architecture:
    1. Load rubric for card type (backend/frontend/architecture)
    2. Build evaluation prompt with code artifacts + rubric
    3. Call LLM with cached rubric (90% cost reduction)
    4. Parse structured JSON response
    5. Calculate weighted score
    6. Generate detailed feedback

    Passing Threshold: 8.0/10 (80% weighted score)
    """

    RUBRICS_DIR = Path(__file__).parent.parent / 'rubrics'
    PASSING_THRESHOLD = 8.0  # 80% weighted score
    MAX_CODE_CHARS = 20000  # Limit code size for LLM context

    def __init__(self):
        """Initialize LLM-as-Judge agent"""
        self.llm_client = get_cached_client() if CACHED_CLIENT_AVAILABLE else None

        if not self.llm_client:
            logger.warning("âš ï¸ LLM client unavailable. Evaluations will be skipped.")
        else:
            logger.info("âœ… LLM-as-Judge Agent initialized")

    def evaluate_code_quality(
        self,
        card_id: str,
        card_type: str,  # 'backend', 'frontend', 'architecture'
        artifacts: Dict[str, Dict[str, str]],
        context: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Evaluate code quality using LLM-based rubric scoring

        Args:
            card_id: Card ID (e.g., 'PROD-001')
            card_type: Type of card ('backend', 'frontend', 'architecture')
            artifacts: {
                'code': {'file.py': content, ...},
                'tests': {'test_file.py': content, ...},
                'docs': {'README.md': content, ...}
            }
            context: Optional context (requirements, design docs)

        Returns:
            {
                'passed': bool,
                'overall_score': float,      # 0-10
                'weighted_score': float,     # 0-10
                'scores': [                  # Per-criterion scores
                    {
                        'criterion': str,
                        'score': float,      # 0-10
                        'weight': float,
                        'justification': str,
                        'evidence': [str],
                        'improvement': str
                    }
                ],
                'summary': {
                    'assessment': str,
                    'strengths': [str],
                    'weaknesses': [str],
                    'priorities': [str]      # Actionable improvement priorities
                },
                'feedback': str,             # Formatted feedback for correction card
                'metadata': {
                    'evaluation_time_ms': int,
                    'model': str,
                    'criteria_count': int,
                    'cache_hit_rate': float,
                    'code_size_chars': int
                }
            }
        """
        start_time = datetime.utcnow()
        logger.info(f"ðŸ” Evaluating code quality for {card_id} (type: {card_type})")

        # Load rubric
        rubric = self._load_rubric(card_type)
        if not rubric:
            return self._skip_evaluation(
                card_id=card_id,
                reason=f"No rubric found for type: {card_type}",
                start_time=start_time
            )

        # Check if LLM available
        if not self.llm_client:
            return self._skip_evaluation(
                card_id=card_id,
                reason="LLM client unavailable",
                start_time=start_time
            )

        # Prepare evaluation prompt
        prompt = self._build_evaluation_prompt(
            card_id=card_id,
            card_type=card_type,
            artifacts=artifacts,
            rubric=rubric,
            context=context
        )

        # Call LLM with caching
        try:
            # Format rubric as markdown for caching
            rubric_markdown = self._format_rubric_as_markdown(rubric)

            response = self.llm_client.generate(
                model='claude-sonnet-4-5-20251029',
                system_prompt=self._get_system_prompt(),
                cached_context=[
                    {'name': f'{rubric["name"]} (v{rubric["version"]})', 'content': rubric_markdown}
                ],
                user_message=prompt,
                max_tokens=2048,
                temperature=0.3  # Lower temperature for consistent evaluation
            )

            # Parse response
            evaluation = self._parse_evaluation_response(
                response_text=response['content'],
                rubric=rubric
            )

            # Calculate scores
            weighted_score = self._calculate_weighted_score(
                scores=evaluation['scores'],
                criteria=rubric['criteria']
            )

            overall_score = sum(s['score'] for s in evaluation['scores']) / len(evaluation['scores'])

            # Determine pass/fail
            passed = weighted_score >= self.PASSING_THRESHOLD

            # Generate feedback
            feedback = self._generate_feedback(
                card_id=card_id,
                passed=passed,
                weighted_score=weighted_score,
                evaluation=evaluation
            )

            # Calculate metadata
            code_size = sum(
                len(content)
                for file_dict in artifacts.values()
                for content in file_dict.values()
            )

            evaluation_time_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)

            logger.info(f"âœ… Evaluation complete for {card_id}: {weighted_score:.1f}/10 ({'PASS' if passed else 'FAIL'})")

            return {
                'passed': passed,
                'overall_score': round(overall_score, 2),
                'weighted_score': round(weighted_score, 2),
                'scores': evaluation['scores'],
                'summary': evaluation['summary'],
                'feedback': feedback,
                'metadata': {
                    'evaluation_time_ms': evaluation_time_ms,
                    'model': response['usage'].get('model', 'unknown'),
                    'criteria_count': len(rubric['criteria']),
                    'cache_hit_rate': response.get('cache_hit_rate', 0.0),
                    'code_size_chars': code_size
                }
            }

        except Exception as e:
            logger.error(f"âŒ Evaluation failed for {card_id}: {e}", exc_info=True)
            return self._skip_evaluation(
                card_id=card_id,
                reason=f"Evaluation error: {e}",
                start_time=start_time
            )

    def _load_rubric(self, card_type: str) -> Optional[Dict[str, Any]]:
        """
        Load rubric JSON for card type

        Args:
            card_type: 'backend', 'frontend', or 'architecture'

        Returns:
            Rubric dict or None if not found
        """
        # Map card types to rubric files
        rubric_files = {
            'backend': 'backend_code_quality.json',
            'frontend': 'frontend_code_quality.json',
            'architecture': 'architecture_compliance.json'
        }

        rubric_file = rubric_files.get(card_type.lower())
        if not rubric_file:
            logger.error(f"âŒ Unknown card type: {card_type}")
            return None

        rubric_path = self.RUBRICS_DIR / rubric_file

        if not rubric_path.exists():
            logger.error(f"âŒ Rubric file not found: {rubric_path}")
            return None

        try:
            with open(rubric_path, 'r', encoding='utf-8') as f:
                rubric = json.load(f)

            logger.info(f"âœ… Loaded rubric: {rubric['name']} v{rubric['version']}")
            return rubric

        except Exception as e:
            logger.error(f"âŒ Failed to load rubric {rubric_path}: {e}")
            return None

    def _format_rubric_as_markdown(self, rubric: Dict[str, Any]) -> str:
        """
        Format rubric as markdown for LLM caching

        Args:
            rubric: Rubric dictionary

        Returns:
            Markdown-formatted rubric
        """
        lines = [
            f"# {rubric['name']} (v{rubric['version']})",
            "",
            f"**Description**: {rubric.get('description', 'N/A')}",
            f"**Scale**: {rubric['scale']}",
            f"**Passing Threshold**: {rubric['passing_threshold']}/10",
            "",
            "## Criteria",
            ""
        ]

        for i, criterion in enumerate(rubric['criteria'], 1):
            lines.extend([
                f"### {i}. {criterion['name']} (weight: {criterion['weight']})",
                "",
                f"**Description**: {criterion['description']}",
                "",
                "**Scoring Levels**:",
                ""
            ])

            for level, description in criterion.get('levels', {}).items():
                lines.append(f"- **{level}**: {description}")

            if 'examples' in criterion:
                lines.extend([
                    "",
                    "**Examples**:",
                    ""
                ])
                for example_type, example_text in criterion['examples'].items():
                    lines.append(f"- *{example_type.title()}*: {example_text}")

            lines.append("")

        return "\n".join(lines)

    def _build_evaluation_prompt(
        self,
        card_id: str,
        card_type: str,
        artifacts: Dict[str, Dict[str, str]],
        rubric: Dict[str, Any],
        context: Optional[str] = None
    ) -> str:
        """
        Build evaluation prompt with code artifacts and rubric

        Args:
            card_id: Card ID
            card_type: Card type
            artifacts: Code, tests, docs
            rubric: Rubric dict
            context: Optional context

        Returns:
            Formatted prompt string
        """
        lines = [
            f"# Code Quality Evaluation for {card_id}",
            "",
            f"**Card Type**: {card_type}",
            ""
        ]

        if context:
            lines.extend([
                "## Context",
                "",
                context,
                ""
            ])

        # Add code artifacts
        for artifact_type, files in artifacts.items():
            if not files:
                continue

            lines.extend([
                f"## {artifact_type.title()} Artifacts",
                ""
            ])

            for filename, content in files.items():
                # Truncate if too large
                truncated = content[:self.MAX_CODE_CHARS]
                if len(content) > self.MAX_CODE_CHARS:
                    truncated += f"\n\n[... truncated {len(content) - self.MAX_CODE_CHARS} chars ...]"

                lines.extend([
                    f"### {filename}",
                    "",
                    "```",
                    truncated,
                    "```",
                    ""
                ])

        # Add evaluation instructions
        lines.extend([
            "## Evaluation Instructions",
            "",
            f"Evaluate the above code against the **{rubric['name']}** rubric.",
            "",
            "For each criterion:",
            "1. Find specific evidence in the code (quote relevant lines)",
            f"2. Score according to the rubric ({rubric['scale']} scale)",
            "3. Justify your score with concrete observations",
            "4. Suggest ONE specific improvement",
            "",
            "Be objective and consistent. Base scores on explicit evidence.",
            "",
            "Respond with valid JSON matching this structure:",
            "```json",
            "{",
            '  "scores": [',
            "    {",
            '      "criterion": "criterion name",',
            '      "score": number,',
            '      "evidence": ["quote or observation 1", "quote 2"],',
            '      "justification": "why this score",',
            '      "improvement": "specific suggestion"',
            "    }",
            "  ],",
            '  "summary": {',
            '    "assessment": "overall quality summary",',
            '    "strengths": ["strength 1", "strength 2"],',
            '    "weaknesses": ["weakness 1"],',
            '    "priorities": ["most important improvement"]',
            "  }",
            "}",
            "```"
        ])

        return "\n".join(lines)

    def _get_system_prompt(self) -> str:
        """Get system prompt for LLM evaluation"""
        return """You are an expert code evaluator. Assess code quality against specific criteria.

Your role:
- Analyze code objectively and consistently
- Find specific evidence (code quotes) for each score
- Provide actionable improvement suggestions
- Base all judgments on observable code characteristics

Guidelines:
- Be strict but fair (high standards, not nitpicky)
- Prefer specific quotes over vague observations
- Suggest improvements that are concrete and achievable
- Consider the broader context (tests, docs, architecture)

Output format: Valid JSON only (no markdown, no explanations outside JSON)"""

    def _parse_evaluation_response(
        self,
        response_text: str,
        rubric: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Parse LLM evaluation response (JSON)

        Args:
            response_text: LLM response text
            rubric: Rubric for validation

        Returns:
            Parsed evaluation dict
        """
        # Extract JSON from markdown code blocks if present
        json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response_text, re.DOTALL)
        if json_match:
            response_text = json_match.group(1)

        try:
            parsed = json.loads(response_text)

            # Validate structure
            if 'scores' not in parsed or 'summary' not in parsed:
                raise ValueError("Missing 'scores' or 'summary' in response")

            # Add weight to each score
            for score in parsed['scores']:
                criterion_name = score['criterion']
                criterion = next((c for c in rubric['criteria'] if c['name'] == criterion_name), None)
                if criterion:
                    score['weight'] = criterion['weight']
                else:
                    score['weight'] = 1.0  # Default weight

            return parsed

        except json.JSONDecodeError as e:
            logger.error(f"âŒ Failed to parse JSON response: {e}")
            logger.debug(f"Response text:\n{response_text}")
            raise ValueError(f"Invalid JSON response: {e}")

    def _calculate_weighted_score(
        self,
        scores: List[Dict[str, Any]],
        criteria: List[Dict[str, Any]]
    ) -> float:
        """
        Calculate weighted overall score

        Args:
            scores: List of criterion scores
            criteria: List of criteria with weights

        Returns:
            Weighted score (0-10)
        """
        total_weight = sum(c['weight'] for c in criteria)
        weighted_sum = sum(
            score['score'] * score.get('weight', 1.0)
            for score in scores
        )

        return weighted_sum / total_weight if total_weight > 0 else 0.0

    def _generate_feedback(
        self,
        card_id: str,
        passed: bool,
        weighted_score: float,
        evaluation: Dict[str, Any]
    ) -> str:
        """
        Generate detailed feedback for correction card

        Args:
            card_id: Card ID
            passed: Whether evaluation passed
            weighted_score: Weighted score
            evaluation: Parsed evaluation

        Returns:
            Formatted feedback string
        """
        lines = []

        if passed:
            lines.extend([
                f"âœ… APPROVED - Code quality meets standards",
                "",
                f"Card: {card_id}",
                f"Score: {weighted_score:.1f}/10 (threshold: {self.PASSING_THRESHOLD})",
                ""
            ])
        else:
            lines.extend([
                f"âŒ NEEDS IMPROVEMENT - Code quality below threshold",
                "",
                f"Card: {card_id}",
                f"Score: {weighted_score:.1f}/10 (threshold: {self.PASSING_THRESHOLD})",
                ""
            ])

        # Summary
        summary = evaluation.get('summary', {})
        if summary.get('assessment'):
            lines.extend([
                "## Assessment",
                "",
                summary['assessment'],
                ""
            ])

        # Scores breakdown
        lines.extend([
            "## Scores by Criterion",
            ""
        ])

        for score in evaluation.get('scores', []):
            status = "âœ…" if score['score'] >= self.PASSING_THRESHOLD else "âš ï¸"
            lines.extend([
                f"{status} **{score['criterion']}**: {score['score']:.1f}/10 (weight: {score.get('weight', 1.0)})",
                f"  - Justification: {score.get('justification', 'N/A')}",
                ""
            ])

        # Strengths
        strengths = summary.get('strengths', [])
        if strengths:
            lines.extend([
                "## Strengths",
                ""
            ])
            for strength in strengths:
                lines.append(f"- {strength}")
            lines.append("")

        # Weaknesses
        weaknesses = summary.get('weaknesses', [])
        if weaknesses:
            lines.extend([
                "## Weaknesses",
                ""
            ])
            for weakness in weaknesses:
                lines.append(f"- {weakness}")
            lines.append("")

        # Improvement priorities
        priorities = summary.get('priorities', [])
        if priorities:
            lines.extend([
                "## Improvement Priorities",
                "",
                "Please address the following (in order of importance):",
                ""
            ])
            for i, priority in enumerate(priorities, 1):
                lines.append(f"{i}. {priority}")
            lines.append("")

        # Detailed improvements per criterion
        lines.extend([
            "## Detailed Improvements",
            ""
        ])

        for score in evaluation.get('scores', []):
            improvement = score.get('improvement', 'None')
            if improvement and improvement.lower() not in ['none', 'none needed', 'n/a']:
                lines.append(f"- **{score['criterion']}**: {improvement}")

        if passed:
            lines.extend([
                "",
                "âœ… Card approved for QA review.",
                "Human QA will perform final validation."
            ])
        else:
            lines.extend([
                "",
                "âŒ Please address the improvements above and resubmit.",
                "Once updated, the card will be re-evaluated automatically."
            ])

        return "\n".join(lines)

    def _skip_evaluation(
        self,
        card_id: str,
        reason: str,
        start_time: datetime
    ) -> Dict[str, Any]:
        """
        Skip evaluation and return default response

        Args:
            card_id: Card ID
            reason: Reason for skipping
            start_time: Evaluation start time

        Returns:
            Default evaluation response (skipped)
        """
        evaluation_time_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)

        logger.warning(f"â­ï¸ Skipping evaluation for {card_id}: {reason}")

        return {
            'passed': True,  # Don't block progress if LLM unavailable
            'overall_score': 0.0,
            'weighted_score': 0.0,
            'scores': [],
            'summary': {
                'assessment': f"Evaluation skipped: {reason}",
                'strengths': [],
                'weaknesses': [],
                'priorities': []
            },
            'feedback': f"â­ï¸ Automated evaluation skipped: {reason}\n\nCard will proceed to manual QA review.",
            'metadata': {
                'evaluation_time_ms': evaluation_time_ms,
                'model': 'none',
                'criteria_count': 0,
                'cache_hit_rate': 0.0,
                'code_size_chars': 0,
                'skipped': True,
                'skip_reason': reason
            }
        }


if __name__ == '__main__':
    # Quick test
    logging.basicConfig(level=logging.INFO)

    agent = LLMJudgeAgent()

    # Test with sample backend code
    sample_code = '''
def authenticate_user(username: str, password: str) -> dict:
    """
    Authenticate user with username and password

    Args:
        username: User's username
        password: User's password

    Returns:
        dict with user info and token

    Raises:
        ValueError: If credentials invalid
    """
    if not username or not password:
        raise ValueError("Username and password required")

    # Hash password
    password_hash = bcrypt.hashpw(password.encode(), bcrypt.gensalt())

    # Query database
    user = db.query(User).filter_by(username=username).first()

    if not user or not bcrypt.checkpw(password.encode(), user.password_hash):
        raise ValueError("Invalid credentials")

    # Generate token
    token = jwt.encode({'user_id': user.id}, SECRET_KEY, algorithm='HS256')

    return {'user': user.to_dict(), 'token': token}
'''

    result = agent.evaluate_code_quality(
        card_id='TEST-001',
        card_type='backend',
        artifacts={
            'code': {'auth.py': sample_code},
            'tests': {},
            'docs': {}
        }
    )

    print(f"\nResult: {'PASS' if result['passed'] else 'FAIL'}")
    print(f"Score: {result['weighted_score']:.1f}/10")
    print(f"\nFeedback:\n{result['feedback']}")
